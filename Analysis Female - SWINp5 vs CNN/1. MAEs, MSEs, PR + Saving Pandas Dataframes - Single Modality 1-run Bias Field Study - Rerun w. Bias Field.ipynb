{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import gaussian_kde\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorboard as tb\n",
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "from random import choices\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def age_debiasing(predicted_ages, chronological_ages):\n",
    "    \n",
    "    predicted_ages = predicted_ages.to_numpy()\n",
    "    chronological_ages = chronological_ages.to_numpy()\n",
    "    \n",
    "    ones = np.ones(len(chronological_ages))\n",
    "    chronological_ages_squared = np.square(chronological_ages)\n",
    "    chronological_ages_confounds = np.array([ones, chronological_ages])\n",
    "    \n",
    "    ones_predicted = np.ones(len(predicted_ages))\n",
    "    predicted_ages_squared = np.square(predicted_ages)\n",
    "    predicted_ages_confounds = np.array([ones_predicted, predicted_ages])\n",
    "    \n",
    "    predicted_ages = np.reshape(predicted_ages, (len(predicted_ages), 1) )\n",
    "    beta = np.dot( np.linalg.pinv(chronological_ages_confounds.T), predicted_ages )\n",
    "\n",
    "    corrected_predicted_ages = (predicted_ages.flatten() - beta[0])/beta[1]\n",
    "    \n",
    "    return corrected_predicted_ages\n",
    "\n",
    "def age_debiasing(predicted_ages, chronological_ages):\n",
    "    \n",
    "    predicted_ages = predicted_ages.to_numpy()\n",
    "    chronological_ages = chronological_ages.to_numpy()\n",
    "    \n",
    "    ones = np.ones(len(chronological_ages))\n",
    "    chronological_ages_squared = np.square(chronological_ages)\n",
    "    chronological_ages_confounds = np.array([ones, chronological_ages])\n",
    "    \n",
    "    ones_predicted = np.ones(len(predicted_ages))\n",
    "    predicted_ages_squared = np.square(predicted_ages)\n",
    "    predicted_ages_confounds = np.array([ones_predicted, predicted_ages])\n",
    "    \n",
    "    predicted_ages = np.reshape(predicted_ages, (len(predicted_ages), 1) )\n",
    "    beta = np.dot( np.linalg.pinv(chronological_ages_confounds.T), predicted_ages )\n",
    "\n",
    "    corrected_predicted_ages = (predicted_ages.flatten() - beta[0])/beta[1]\n",
    "    \n",
    "    return corrected_predicted_ages\n",
    "\n",
    "def tensorboard_reader(directory, tag='AgeDelta/epoch'):\n",
    "    \"\"\"\n",
    "    Sources: \n",
    "    https://stackoverflow.com/questions/41074688/how-do-you-read-tensorboard-files-programmatically\n",
    "    https://stackoverflow.com/questions/37304461/tensorflow-importing-data-from-a-tensorboard-tfevent-file\n",
    "    \"\"\"\n",
    "    \n",
    "    for dataset in ['train', 'validation']:\n",
    "    \n",
    "        dct = {}\n",
    "        steps = []\n",
    "        values = []\n",
    "    \n",
    "        if dataset == 'train':\n",
    "            dir_name = directory + '/train/'\n",
    "        else:\n",
    "            dir_name = directory + '/validation/'\n",
    "\n",
    "        list_of_files = os.listdir(dir_name)\n",
    "\n",
    "        for file in list_of_files:\n",
    "            file_name=dir_name+file\n",
    "#             print(file_name)\n",
    "            ea = event_accumulator.EventAccumulator(file_name,\n",
    "              size_guidance={\n",
    "              event_accumulator.COMPRESSED_HISTOGRAMS: 500,\n",
    "              event_accumulator.IMAGES: 4,\n",
    "              event_accumulator.AUDIO: 4,\n",
    "              event_accumulator.SCALARS: 0,\n",
    "              event_accumulator.HISTOGRAMS: 1,\n",
    "          })\n",
    "            ea.Reload()\n",
    "#             print(ea.Tags())\n",
    "            if tag in ea.Tags()['scalars']:\n",
    "                for e in ea.Scalars(tag):\n",
    "                    step = e.step\n",
    "                    value = e.value\n",
    "                    steps.append(step)\n",
    "                    values.append(value)\n",
    "\n",
    "        dct['step'] = steps\n",
    "        \n",
    "        if dataset == 'train':\n",
    "            dct['train'] = values\n",
    "            dfp = pd.DataFrame.from_dict(dct)\n",
    "        else:\n",
    "            dct['validation'] = values\n",
    "            dfv = pd.DataFrame.from_dict(dct)\n",
    "\n",
    "    df = pd.merge(dfp, dfv, how='left', on='step')\n",
    "    df = df.sort_values('step')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def linear_plotter(x, y_train, y_validation, xlabel, ylabel, title, ylim_flag=False, xlim=None):\n",
    "    \n",
    "    if isinstance(x, np.ndarray):\n",
    "        pass\n",
    "    else:\n",
    "        x = x.values\n",
    "        y_train = y_train.values\n",
    "        y_validation = y_validation.values\n",
    "    \n",
    "    plt.figure(figsize=[14.4,7.2])\n",
    "    plt.plot(x, y_train, label='Train')\n",
    "    plt.plot(x, y_validation, label='Validation')\n",
    "    plt.xlabel(xlabel)\n",
    "    if ylim_flag == True:\n",
    "        ylim_min = -0.25\n",
    "        ylim_max = np.max(y_train) + 5\n",
    "        plt.ylim((ylim_min, ylim_max))\n",
    "    best_validation = np.min(y_validation)\n",
    "    best_epoch = x[np.where(y_validation == np.min(y_validation))[0][0]]\n",
    "    best_train = y_train[np.where(y_validation == np.min(y_validation))[0][0]]\n",
    "    gap = best_train - best_validation\n",
    "    print(best_epoch, best_validation)\n",
    "    print(-gap)\n",
    "    plt.scatter(x=best_epoch, y=best_validation, s=100, marker='*', color='red', label='Selected Best Epoch')\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    if xlim is not None:\n",
    "        plt.xlim(xlim)\n",
    "    plt.show()\n",
    "    \n",
    "def linear_plotter_comparison(Xs, Ys, Labels,\n",
    "                              xlabel, ylabel, title, ylim_flag=False):\n",
    "    \n",
    "    plt.figure(figsize=[7,5])\n",
    "    for idx, x in enumerate(Xs):\n",
    "        plt.plot(x, Ys[idx], label=Labels[idx])\n",
    "    plt.xlabel(xlabel)\n",
    "    if ylim_flag == True:\n",
    "        ylim_min = -0.25\n",
    "        y_max = [np.max(Ys[idx]) for idx in range(len(Ys))]\n",
    "        ylim_max = min(y_max)\n",
    "        plt.ylim((ylim_min, ylim_max))\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def plot_everything(network='49', ylim2=[2.175, 2.45], control=False):\n",
    "    dfs = []\n",
    "    names = []\n",
    "    range_points = [1,2,3]\n",
    "#     network = '49'\n",
    "    preambul = 'MM'\n",
    "    if control == True:\n",
    "        preambul = 'AM'\n",
    "        range_points = [10,11,12]\n",
    "\n",
    "    tag = 'AgeDelta/epoch'\n",
    "    for i in range_points:\n",
    "        name = preambul + str(int(i)) + '-' + network\n",
    "        directory = '../logs/' + name\n",
    "        df = tensorboard_reader(directory=directory, tag=tag)\n",
    "        dfs.append(df)\n",
    "        names.append(name)\n",
    "\n",
    "    dfs2 = []\n",
    "    names2 = []\n",
    "    range_points = [1,2,3]\n",
    "#     network = '49'\n",
    "\n",
    "    tag2 = 'LearningRate/iteration'\n",
    "    for i in range_points:\n",
    "        name2 = preambul + str(int(i)) + '-' + network\n",
    "        directory2 = '../logs/' + name\n",
    "    #     if os.path.isdir(directory):\n",
    "        df2 = tensorboard_reader(directory=directory2, tag=tag2)\n",
    "        dfs2.append(df2)\n",
    "        names2.append(name2)\n",
    "\n",
    "    for i, df in enumerate(dfs):\n",
    "        name = names[i]\n",
    "        df = df.dropna(axis=0)\n",
    "        linear_plotter(x=df.step, y_train=df.train, y_validation=df.validation, \n",
    "                       xlabel='Epochs', ylabel=tag, title=name, xlim=[-2, 300])\n",
    "\n",
    "    Xs = [dfs[i].step.values for i in range(len(dfs))]\n",
    "    Ys = [dfs[i].validation.values for i in range(len(dfs))]\n",
    "    Labels = [preambul + str(int(i)) + '-' + network for i in range_points]\n",
    "    xlabel='Epochs'\n",
    "    ylabel='AgeDelta/epoch'\n",
    "\n",
    "    plt.figure(figsize=[14.4,7.2])\n",
    "    for idx, x in enumerate(Xs):\n",
    "        y = Ys[idx]\n",
    "        x = np.delete(x, np.argwhere(np.isnan(y)))\n",
    "        y = np.delete(y, np.argwhere(np.isnan(y)))\n",
    "        plt.plot(x, y, label=Labels[idx], alpha=0.3)\n",
    "        best_validation = np.min(y)\n",
    "        best_epoch = x[np.where(y == np.min(y))[0][0]]\n",
    "        label = 'Selected Best Epoch for ' + Labels[idx]\n",
    "        if idx < 3:\n",
    "            plt.scatter(x=best_epoch, y=best_validation, s=150, marker='*', label=label)\n",
    "        else:\n",
    "            plt.scatter(x=best_epoch, y=best_validation, s=150, marker='o', label=label)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.legend()\n",
    "    plt.ylim(ylim2)\n",
    "    plt.xlim([-1,300])\n",
    "    \n",
    "    print('---------------------------------------------------------------------------------------------------')\n",
    "    print('---------------------------------------------------------------------------------------------------')\n",
    "    print('---------------------------------------------------------------------------------------------------')\n",
    "\n",
    "#     for idx in range(len(dfs2)):\n",
    "#         dfs2[idx].step = dfs2[idx].step / (dfs2[idx].step.max() / dfs[idx].step.max())\n",
    "\n",
    "#     ncols=2\n",
    "#     nrows=1\n",
    "#     sz=7.2\n",
    "#     title_font_size=22\n",
    "#     ticks_font_size=18\n",
    "#     legend_font = 18\n",
    "\n",
    "#     for idx in range(len(dfs)):\n",
    "\n",
    "#         fig, axes = plt.subplots(ncols=ncols,nrows=nrows,figsize=(ncols*sz,nrows*sz))\n",
    "\n",
    "#         axes = axes.ravel()\n",
    "\n",
    "#         axes[0] = plt.subplot()\n",
    "#         ax2 = axes[0].twinx()\n",
    "#         l1, = axes[0].plot(dfs[idx].step, dfs[idx].validation, color='tab:orange')\n",
    "#         l2, = ax2.plot(dfs2[idx].step, dfs2[idx].train, linestyle='dotted', color='b')\n",
    "\n",
    "#         best_validation = np.min(dfs[idx].validation.values)\n",
    "#         best_epoch = dfs[idx].step.values[np.where(dfs[idx].validation.values == best_validation)[0][0]]\n",
    "#         l3 = axes[0].scatter(x=best_epoch, y=best_validation, s=100, marker='*', color='red')\n",
    "\n",
    "\n",
    "#         plt.legend([l1, l2, l3], ['Val Curve', 'LR', 'Selected Best Epoch'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         axes[0].set_ylabel('Age Delta')\n",
    "#         ax2.set_ylabel('Learning Rate')\n",
    "#         axes[0].set_xlabel('Training Epochs')\n",
    "#         ax2.grid(linestyle='--')\n",
    "\n",
    "#         axes[0].set_xlim([-2, 300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls '../../agemapper/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('../../agemapper/Analsysis Female New/female_test.pkl')\n",
    "df_test = {}\n",
    "df_test['Unnamed: 0'] = df.iloc[0].dataframe['Unnamed: 0'].to_list()\n",
    "for idx in range(len(df)):\n",
    "    name = df.iloc[idx].modality\n",
    "    data = df.iloc[idx].dataframe.mean_output_age.to_list()\n",
    "    df_test[name] = data\n",
    "    \n",
    "df_test = pd.DataFrame.from_dict(df_test)\n",
    "df_test['target_age'] = df.iloc[0].dataframe.target_age.to_list()\n",
    "\n",
    "df_testA, df_testB = train_test_split(df_test, test_size=0.5, random_state=1)\n",
    "subjectsA = df_testA['Unnamed: 0'].to_list()\n",
    "subjectsB = df_testB['Unnamed: 0'].to_list()\n",
    "del df_testA, df_testB, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# CNN\n",
    "\n",
    "net1 = \"NC2\" # T1NL\n",
    "# net2 = 'NC0-5' # SWI\n",
    "# net3 = 'NC0-4' # FA\n",
    "# net4 = 'NC0-3' # rsfMRI0\n",
    "# net5 = 'NC0-2' # tfMRI-1\n",
    "# net6 = 'NC0-1' # dmri tracts\n",
    "\n",
    "\n",
    "\n",
    "# net19 = '11' # tfMRI 5\n",
    "\n",
    "dfs = [\n",
    "    net1,\n",
    "#     net2,\n",
    "#     net3,\n",
    "#     net4,\n",
    "#     net5,\n",
    "#     net6,\n",
    "]\n",
    "\n",
    "names = [\n",
    "\n",
    "    'T1_nonlinear',           #1\n",
    "#     'swi',                    #2\n",
    "#     'tbss_FA',                #3\n",
    "#     'rsfmri_0',               #4\n",
    "#     'tfmri_1',                #5\n",
    "#     'tracts',                 #6\n",
    "#     'T2_lessions',            #7\n",
    "    \n",
    "]\n",
    "\n",
    "noiseSTD = [\n",
    "    0,0.4,0.8,1.2,1.6,2\n",
    "]\n",
    "    \n",
    "    \n",
    "dfs1, dfs2, dfs3 = [], [], []\n",
    "dfs4, dfs5, dfs6 = [], [], []\n",
    "# dfs7, dfs8, dfs9 = [], [], []\n",
    "# dfs10, dfs11, dfs12 = [], [], []\n",
    "\n",
    "dfs1 = dfs.copy()\n",
    "dfs2 = dfs.copy()\n",
    "dfs3 = dfs.copy()\n",
    "dfs4 = dfs.copy()\n",
    "dfs5 = dfs.copy()\n",
    "dfs6 = dfs.copy()\n",
    "# dfs7 = dfs.copy()\n",
    "# dfs8 = dfs.copy()\n",
    "# dfs9 = dfs.copy()\n",
    "# dfs10 = dfs.copy()\n",
    "# dfs11 = dfs.copy()\n",
    "# dfs12 = dfs.copy()\n",
    "\n",
    "\n",
    "for idx in range(len(dfs)):\n",
    "    \n",
    "    dfs1[idx] = '../predictions/' + dfs1[idx] + '-6_predictions/output_statistics_test.csv' \n",
    "    dfs2[idx] = '../predictions/' + dfs2[idx] + '-5_predictions/output_statistics_test.csv'\n",
    "    dfs3[idx] = '../predictions/' + dfs3[idx] + '-4_predictions/output_statistics_test.csv'\n",
    "    dfs4[idx] = '../predictions/' + dfs4[idx] + '-3_predictions/output_statistics_test.csv'\n",
    "    dfs5[idx] = '../predictions/' + dfs5[idx] + '-2_predictions/output_statistics_test.csv'\n",
    "    dfs6[idx] = '../predictions/' + dfs6[idx] + '-1_predictions/output_statistics_test.csv'\n",
    "#     dfs7[idx] = '../predictions/' + dfs7[idx] + '_noise_predictions/output_statistics_test.csv' + '_0.1'\n",
    "#     dfs8[idx] = '../predictions/' + dfs8[idx] + '_noise_predictions/output_statistics_test.csv' + '_0.12'\n",
    "#     dfs9[idx] = '../predictions/' + dfs9[idx] + '_noise_predictions/output_statistics_test.csv' + '_0.14'\n",
    "#     dfs10[idx] = '../predictions/' + dfs10[idx] + '_noise_predictions/output_statistics_test.csv' + '_0.16'\n",
    "#     dfs11[idx] = '../predictions/' + dfs11[idx] + '_noise_predictions/output_statistics_test.csv' + '_0.18'\n",
    "#     dfs12[idx] = '../predictions/' + dfs12[idx] + '_noise_predictions/output_statistics_test.csv' + '_0.2'\n",
    "\n",
    "    \n",
    "for idx in range(len(dfs)):\n",
    "    dfs1[idx] = pd.read_csv(dfs1[idx])\n",
    "    dfs2[idx] = pd.read_csv(dfs2[idx])\n",
    "    dfs3[idx] = pd.read_csv(dfs3[idx])\n",
    "    dfs4[idx] = pd.read_csv(dfs4[idx])\n",
    "    dfs5[idx] = pd.read_csv(dfs5[idx])\n",
    "    dfs6[idx] = pd.read_csv(dfs6[idx])\n",
    "#     dfs7[idx] = pd.read_csv(dfs7[idx])\n",
    "#     dfs8[idx] = pd.read_csv(dfs8[idx])\n",
    "#     dfs9[idx] = pd.read_csv(dfs9[idx])\n",
    "#     dfs10[idx] = pd.read_csv(dfs10[idx])\n",
    "#     dfs11[idx] = pd.read_csv(dfs11[idx])\n",
    "#     dfs12[idx] = pd.read_csv(dfs12[idx])\n",
    "    \n",
    "\n",
    "subjects = subjectsB\n",
    "\n",
    "for idx in range(len(dfs)):\n",
    "    for subj in dfs1[idx]['Unnamed: 0'].to_list():\n",
    "        if subj not in subjects:\n",
    "            dfs1[idx] = dfs1[idx].drop(dfs1[idx][dfs1[idx]['Unnamed: 0'] == subj].index)\n",
    "            dfs2[idx] = dfs2[idx].drop(dfs2[idx][dfs2[idx]['Unnamed: 0'] == subj].index)\n",
    "            dfs3[idx] = dfs3[idx].drop(dfs3[idx][dfs3[idx]['Unnamed: 0'] == subj].index)\n",
    "            dfs4[idx] = dfs4[idx].drop(dfs4[idx][dfs4[idx]['Unnamed: 0'] == subj].index)\n",
    "            dfs5[idx] = dfs5[idx].drop(dfs5[idx][dfs5[idx]['Unnamed: 0'] == subj].index)\n",
    "            dfs6[idx] = dfs6[idx].drop(dfs6[idx][dfs6[idx]['Unnamed: 0'] == subj].index)\n",
    "#             dfs7[idx] = dfs7[idx].drop(dfs7[idx][dfs7[idx]['Unnamed: 0'] == subj].index)\n",
    "#             dfs8[idx] = dfs8[idx].drop(dfs8[idx][dfs8[idx]['Unnamed: 0'] == subj].index)\n",
    "#             dfs9[idx] = dfs9[idx].drop(dfs9[idx][dfs9[idx]['Unnamed: 0'] == subj].index)\n",
    "#             dfs10[idx] = dfs10[idx].drop(dfs10[idx][dfs10[idx]['Unnamed: 0'] == subj].index)\n",
    "#             dfs11[idx] = dfs11[idx].drop(dfs11[idx][dfs11[idx]['Unnamed: 0'] == subj].index)\n",
    "#             dfs12[idx] = dfs12[idx].drop(dfs12[idx][dfs12[idx]['Unnamed: 0'] == subj].index)\n",
    "    \n",
    "    \n",
    "for idx in range(len(dfs)):\n",
    "    dfs[idx] = pd.merge(dfs1[idx], dfs2[idx], on='Unnamed: 0', how='inner')\n",
    "    dfs[idx] = pd.merge(dfs[idx], dfs3[idx], on='Unnamed: 0', how='inner')\n",
    "    \n",
    "    dfs[idx].rename({\n",
    "        'target_age_x': 'target_age_original',\n",
    "        'output_age_x': 'output_age_1',\n",
    "        'age_delta_x': 'age_delta_1',\n",
    "        'loss_x': 'loss_1',\n",
    "        'target_age_y': 'target_age_2',\n",
    "        'output_age_y': 'output_age_2',\n",
    "        'age_delta_y': 'age_delta_2',\n",
    "        'loss_y': 'loss_2',\n",
    "        'target_age': 'target_age_3',\n",
    "        'output_age': 'output_age_3',\n",
    "        'age_delta': 'age_delta_3',\n",
    "        'loss': 'loss_3',\n",
    "    }, axis=1, inplace=True)\n",
    "    \n",
    "    dfs[idx].drop(['target_age_2', 'target_age_3'], axis=1, inplace=True)\n",
    "    \n",
    "    \n",
    "    dfs[idx] = pd.merge(dfs[idx], dfs4[idx], on='Unnamed: 0', how='inner')\n",
    "    dfs[idx].rename({\n",
    "        'output_age': 'output_age_4',\n",
    "        'age_delta': 'age_delta_4',\n",
    "        'loss': 'loss_4',\n",
    "    }, axis=1, inplace=True)\n",
    "    dfs[idx] = dfs[idx].drop(['target_age'], axis=1)\n",
    "    \n",
    "    \n",
    "    dfs[idx] = pd.merge(dfs[idx], dfs5[idx], on='Unnamed: 0', how='inner')\n",
    "    dfs[idx].rename({\n",
    "        'output_age': 'output_age_5',\n",
    "        'age_delta': 'age_delta_5',\n",
    "        'loss': 'loss_5',\n",
    "    }, axis=1, inplace=True)\n",
    "    dfs[idx] = dfs[idx].drop(['target_age'], axis=1)\n",
    "    \n",
    "    dfs[idx] = pd.merge(dfs[idx], dfs6[idx], on='Unnamed: 0', how='inner')\n",
    "    dfs[idx].rename({\n",
    "        'output_age': 'output_age_6',\n",
    "        'age_delta': 'age_delta_6',\n",
    "        'loss': 'loss_6',\n",
    "    }, axis=1, inplace=True)\n",
    "    dfs[idx] = dfs[idx].drop(['target_age'], axis=1)\n",
    "      \n",
    "    \n",
    "#     dfs[idx] = pd.merge(dfs[idx], dfs7[idx], on='Unnamed: 0', how='inner')\n",
    "#     dfs[idx].rename({\n",
    "#         'output_age': 'output_age_7',\n",
    "#         'age_delta': 'age_delta_7',\n",
    "#         'loss': 'loss_7',\n",
    "#     }, axis=1, inplace=True)\n",
    "#     dfs[idx] = dfs[idx].drop(['target_age'], axis=1)\n",
    "    \n",
    "#     dfs[idx] = pd.merge(dfs[idx], dfs8[idx], on='Unnamed: 0', how='inner')\n",
    "#     dfs[idx].rename({\n",
    "#         'output_age': 'output_age_8',\n",
    "#         'age_delta': 'age_delta_8',\n",
    "#         'loss': 'loss_8',\n",
    "#     }, axis=1, inplace=True)\n",
    "#     dfs[idx] = dfs[idx].drop(['target_age'], axis=1)\n",
    "    \n",
    "#     dfs[idx] = pd.merge(dfs[idx], dfs9[idx], on='Unnamed: 0', how='inner')\n",
    "#     dfs[idx].rename({\n",
    "#         'output_age': 'output_age_9',\n",
    "#         'age_delta': 'age_delta_9',\n",
    "#         'loss': 'loss_9',\n",
    "#     }, axis=1, inplace=True)\n",
    "#     dfs[idx] = dfs[idx].drop(['target_age'], axis=1)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     dfs[idx] = pd.merge(dfs[idx], dfs10[idx], on='Unnamed: 0', how='inner')\n",
    "#     dfs[idx].rename({\n",
    "#         'output_age': 'output_age_10',\n",
    "#         'age_delta': 'age_delta_10',\n",
    "#         'loss': 'loss_10',\n",
    "#     }, axis=1, inplace=True)\n",
    "#     dfs[idx] = dfs[idx].drop(['target_age'], axis=1)\n",
    "    \n",
    "#     dfs[idx] = pd.merge(dfs[idx], dfs11[idx], on='Unnamed: 0', how='inner')\n",
    "#     dfs[idx].rename({\n",
    "#         'output_age': 'output_age_11',\n",
    "#         'age_delta': 'age_delta_11',\n",
    "#         'loss': 'loss_11',\n",
    "#     }, axis=1, inplace=True)\n",
    "#     dfs[idx] = dfs[idx].drop(['target_age'], axis=1)\n",
    "    \n",
    "#     dfs[idx] = pd.merge(dfs[idx], dfs12[idx], on='Unnamed: 0', how='inner')\n",
    "#     dfs[idx].rename({\n",
    "#         'output_age': 'output_age_12',\n",
    "#         'age_delta': 'age_delta_12',\n",
    "#         'loss': 'loss_12',\n",
    "#     }, axis=1, inplace=True)\n",
    "#     dfs[idx] = dfs[idx].drop(['target_age'], axis=1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    dfs[idx].rename({\n",
    "        'target_age_original': 'target_age',\n",
    "    }, axis=1, inplace=True)\n",
    "    \n",
    "#     dfs[idx]['mean_output_age'] = dfs[idx][['output_age_1', 'output_age_2', 'output_age_3',\n",
    "#                                            'output_age_4', 'output_age_5', 'output_age_6',\n",
    "#                                            'output_age_7', 'output_age_8', 'output_age_9',\n",
    "#                                            ]].mean(axis=1)\n",
    "#     dfs[idx]['mean_age_delta'] = dfs[idx]['mean_output_age'] - dfs[idx]['target_age']\n",
    "    \n",
    "for idx in range(len(dfs)):\n",
    "    for idx2 in range(1,7):\n",
    "        key1 = 'output_age_decon_' + str(idx2)\n",
    "        key2 = 'output_age_' + str(idx2)\n",
    "        dfs[idx][key1] = age_debiasing(dfs[idx][key2], dfs[idx].target_age)\n",
    "        key3 = 'age_delta_decon_' + str(idx2)\n",
    "        dfs[idx][key3] = dfs[idx][key1] - dfs[idx].target_age\n",
    "    \n",
    "#     dfs[idx]['output_age_decon'] = age_debiasing(dfs[idx].mean_output_age, dfs[idx].target_age)\n",
    "#     dfs[idx]['age_delta_decon'] = dfs[idx].output_age_decon - dfs[idx].target_age\n",
    "\n",
    "\n",
    "print(\"{:15} : {:7}  |  {:3}  |  {:3}  |  {:3}  |  {:3}  |  {:3}  \".format(\n",
    "    'Mod/Std', '0', \"0.04\", \"0.08\", \"0.12\", '0.16', \"0.2\"))\n",
    "print('-----------------------------------------------------------------------------------------------')\n",
    "\n",
    "for idx in range(len(dfs)):\n",
    "#     _, dfs[idx] = train_test_split(dfs[idx], test_size=0.5, random_state=1)\n",
    "    print(\"{:15} : {:7.3f}  |  {:.3f} |  {:.3f} |  {:.3f}   |  {:.3f} |  {:.3f} |   \".format(\n",
    "        names[idx],\n",
    "        np.abs(dfs[idx].age_delta_1).mean(), \n",
    "        np.abs(dfs[idx].age_delta_2).mean(),\n",
    "        np.abs(dfs[idx].age_delta_3).mean(),\n",
    "        np.abs(dfs[idx].age_delta_4).mean(), \n",
    "        np.abs(dfs[idx].age_delta_5).mean(),\n",
    "        np.abs(dfs[idx].age_delta_6).mean(),\n",
    "#         np.abs(dfs[idx].age_delta_7).mean(), \n",
    "#         np.abs(dfs[idx].age_delta_8).mean(),\n",
    "#         np.abs(dfs[idx].age_delta_9).mean(),\n",
    "#         np.abs(dfs[idx].age_delta_10).mean(), \n",
    "#         np.abs(dfs[idx].age_delta_11).mean(),\n",
    "#         np.abs(dfs[idx].age_delta_12).mean(),\n",
    "    )\n",
    "         )\n",
    "    \n",
    "print('\\n')      \n",
    "\n",
    "# Modality        : Run1     |  Run2  |  Run3  |  Run4  |  Run5  |  Run6  |  Run7  |  Run8  |  Run9  |  MAE  |  \n",
    "# -----------------------------------------------------------------------------------------------\n",
    "# T1_nonlinear    :   2.490  |  2.483 |  2.382 |  2.476   |  2.467 |  2.504 |  2.487 |  2.439 |  2.399 |  2.190   \n",
    "# T2_nonlinear    :   2.330  |  2.361 |  2.314 |  2.349   |  2.327 |  2.323 |  2.297 |  2.316 |  2.356 |  2.086   \n",
    "# tbss_FA         :   2.891  |  2.906 |  2.802 |  2.803   |  2.909 |  2.819 |  2.839 |  2.887 |  2.865 |  2.587   \n",
    "# tbss_MD         :   3.082  |  3.059 |  2.980 |  3.038   |  3.084 |  3.009 |  3.078 |  3.059 |  2.948 |  2.804   \n",
    "# rsfmri_0        :   4.397  |  4.485 |  4.435 |  4.471   |  4.545 |  4.476 |  4.451 |  4.478 |  4.406 |  4.220   \n",
    "# swi             :   3.343  |  3.142 |  3.273 |  3.333   |  3.277 |  3.329 |  3.226 |  3.168 |  3.169 |  2.939   \n",
    "# tfmri_1         :   3.802  |  3.834 |  3.764 |  3.703   |  3.691 |  3.742 |  3.746 |  3.742 |  3.810 |  3.407   \n",
    "# tracts          :   3.357  |  3.309 |  3.313 |  3.319   |  3.292 |  3.271 |  3.395 |  3.376 |  3.346 |  3.070   \n",
    "# tbss_MO         :   3.160  |  3.140 |  3.060 |  3.169   |  3.098 |  3.132 |  3.164 |  3.174 |  3.066 |  2.873   \n",
    "# tbss_ISOVF_s    :   3.490  |  3.478 |  3.587 |  3.421   |  3.431 |  3.494 |  3.505 |  3.479 |  3.693 |  3.286   \n",
    "# tbss_L2         :   3.052  |  2.984 |  2.992 |  2.992   |  2.977 |  2.954 |  3.038 |  3.132 |  3.024 |  2.806   \n",
    "# rsfmri_2        :   4.399  |  4.404 |  4.498 |  4.464   |  4.547 |  4.643 |  4.377 |  4.612 |  4.426 |  4.159   \n",
    "# rsfmri_10       :   4.714  |  4.539 |  4.479 |  4.515   |  4.477 |  4.560 |  4.506 |  4.553 |  4.512 |  4.243   \n",
    "# rsfmri_5        :   4.408  |  4.390 |  4.315 |  4.291   |  4.392 |  4.376 |  4.405 |  4.424 |  4.414 |  4.160   \n",
    "# rsfmri_21       :   4.529  |  4.398 |  4.515 |  4.465   |  4.244 |  4.366 |  4.303 |  4.340 |  4.424 |  4.060   \n",
    "# T2_lessions     :   4.080  |  4.266 |  4.113 |  4.069   |  4.038 |  4.014 |  4.023 |  4.145 |  4.074 |  3.936   \n",
    "# tbss_FA_s       :   3.084  |  3.175 |  3.049 |  2.992   |  3.080 |  3.102 |  3.074 |  3.032 |  3.156 |  2.903   \n",
    "# tbss_L3_s       :   3.380  |  3.345 |  3.281 |  3.404   |  3.266 |  3.294 |  3.258 |  3.312 |  3.317 |  3.132   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'modality':names, 'dataframe':dfs}\n",
    "df = pd.DataFrame(data=d)\n",
    "df.to_pickle('female_test_testB_CNN_RandomBiasField_retrain.pkl', protocol = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SWIN\n",
    "\n",
    "net1 = \"NS2\" # T1NL\n",
    "# net2 = 'NC0-5' # SWI\n",
    "# net3 = 'NC0-4' # FA\n",
    "# net4 = 'NC0-3' # rsfMRI0\n",
    "# net5 = 'NC0-2' # tfMRI-1\n",
    "# net6 = 'NC0-1' # dmri tracts\n",
    "\n",
    "\n",
    "\n",
    "# net19 = '11' # tfMRI 5\n",
    "\n",
    "dfs = [\n",
    "    net1,\n",
    "#     net2,\n",
    "#     net3,\n",
    "#     net4,\n",
    "#     net5,\n",
    "#     net6,\n",
    "]\n",
    "\n",
    "names = [\n",
    "\n",
    "    'T1_nonlinear',           #1\n",
    "#     'swi',                    #2\n",
    "#     'tbss_FA',                #3\n",
    "#     'rsfmri_0',               #4\n",
    "#     'tfmri_1',                #5\n",
    "#     'tracts',                 #6\n",
    "#     'T2_lessions',            #7\n",
    "    \n",
    "]\n",
    "\n",
    "noiseSTD = [\n",
    "    0,0.4,0.8,1.2,1.6,2\n",
    "]\n",
    "    \n",
    "    \n",
    "dfs1, dfs2, dfs3 = [], [], []\n",
    "dfs4, dfs5, dfs6 = [], [], []\n",
    "# dfs7, dfs8, dfs9 = [], [], []\n",
    "# dfs10, dfs11, dfs12 = [], [], []\n",
    "\n",
    "dfs1 = dfs.copy()\n",
    "dfs2 = dfs.copy()\n",
    "dfs3 = dfs.copy()\n",
    "dfs4 = dfs.copy()\n",
    "dfs5 = dfs.copy()\n",
    "dfs6 = dfs.copy()\n",
    "# dfs7 = dfs.copy()\n",
    "# dfs8 = dfs.copy()\n",
    "# dfs9 = dfs.copy()\n",
    "# dfs10 = dfs.copy()\n",
    "# dfs11 = dfs.copy()\n",
    "# dfs12 = dfs.copy()\n",
    "\n",
    "\n",
    "for idx in range(len(dfs)):\n",
    "    \n",
    "    dfs1[idx] = '../predictions/' + dfs1[idx] + '-6_predictions/output_statistics_test.csv' \n",
    "    dfs2[idx] = '../predictions/' + dfs2[idx] + '-5_predictions/output_statistics_test.csv'\n",
    "    dfs3[idx] = '../predictions/' + dfs3[idx] + '-4_predictions/output_statistics_test.csv'\n",
    "    dfs4[idx] = '../predictions/' + dfs4[idx] + '-3_predictions/output_statistics_test.csv'\n",
    "    dfs5[idx] = '../predictions/' + dfs5[idx] + '-2_predictions/output_statistics_test.csv'\n",
    "    dfs6[idx] = '../predictions/' + dfs6[idx] + '-1_predictions/output_statistics_test.csv'\n",
    "#     dfs7[idx] = '../predictions/' + dfs7[idx] + '_noise_predictions/output_statistics_test.csv' + '_0.1'\n",
    "#     dfs8[idx] = '../predictions/' + dfs8[idx] + '_noise_predictions/output_statistics_test.csv' + '_0.12'\n",
    "#     dfs9[idx] = '../predictions/' + dfs9[idx] + '_noise_predictions/output_statistics_test.csv' + '_0.14'\n",
    "#     dfs10[idx] = '../predictions/' + dfs10[idx] + '_noise_predictions/output_statistics_test.csv' + '_0.16'\n",
    "#     dfs11[idx] = '../predictions/' + dfs11[idx] + '_noise_predictions/output_statistics_test.csv' + '_0.18'\n",
    "#     dfs12[idx] = '../predictions/' + dfs12[idx] + '_noise_predictions/output_statistics_test.csv' + '_0.2'\n",
    "\n",
    "    \n",
    "for idx in range(len(dfs)):\n",
    "    dfs1[idx] = pd.read_csv(dfs1[idx])\n",
    "    dfs2[idx] = pd.read_csv(dfs2[idx])\n",
    "    dfs3[idx] = pd.read_csv(dfs3[idx])\n",
    "    dfs4[idx] = pd.read_csv(dfs4[idx])\n",
    "    dfs5[idx] = pd.read_csv(dfs5[idx])\n",
    "    dfs6[idx] = pd.read_csv(dfs6[idx])\n",
    "#     dfs7[idx] = pd.read_csv(dfs7[idx])\n",
    "#     dfs8[idx] = pd.read_csv(dfs8[idx])\n",
    "#     dfs9[idx] = pd.read_csv(dfs9[idx])\n",
    "#     dfs10[idx] = pd.read_csv(dfs10[idx])\n",
    "#     dfs11[idx] = pd.read_csv(dfs11[idx])\n",
    "#     dfs12[idx] = pd.read_csv(dfs12[idx])\n",
    "    \n",
    "\n",
    "subjects = subjectsB\n",
    "\n",
    "for idx in range(len(dfs)):\n",
    "    for subj in dfs1[idx]['Unnamed: 0'].to_list():\n",
    "        if subj not in subjects:\n",
    "            dfs1[idx] = dfs1[idx].drop(dfs1[idx][dfs1[idx]['Unnamed: 0'] == subj].index)\n",
    "            dfs2[idx] = dfs2[idx].drop(dfs2[idx][dfs2[idx]['Unnamed: 0'] == subj].index)\n",
    "            dfs3[idx] = dfs3[idx].drop(dfs3[idx][dfs3[idx]['Unnamed: 0'] == subj].index)\n",
    "            dfs4[idx] = dfs4[idx].drop(dfs4[idx][dfs4[idx]['Unnamed: 0'] == subj].index)\n",
    "            dfs5[idx] = dfs5[idx].drop(dfs5[idx][dfs5[idx]['Unnamed: 0'] == subj].index)\n",
    "            dfs6[idx] = dfs6[idx].drop(dfs6[idx][dfs6[idx]['Unnamed: 0'] == subj].index)\n",
    "#             dfs7[idx] = dfs7[idx].drop(dfs7[idx][dfs7[idx]['Unnamed: 0'] == subj].index)\n",
    "#             dfs8[idx] = dfs8[idx].drop(dfs8[idx][dfs8[idx]['Unnamed: 0'] == subj].index)\n",
    "#             dfs9[idx] = dfs9[idx].drop(dfs9[idx][dfs9[idx]['Unnamed: 0'] == subj].index)\n",
    "#             dfs10[idx] = dfs10[idx].drop(dfs10[idx][dfs10[idx]['Unnamed: 0'] == subj].index)\n",
    "#             dfs11[idx] = dfs11[idx].drop(dfs11[idx][dfs11[idx]['Unnamed: 0'] == subj].index)\n",
    "#             dfs12[idx] = dfs12[idx].drop(dfs12[idx][dfs12[idx]['Unnamed: 0'] == subj].index)\n",
    "    \n",
    "    \n",
    "for idx in range(len(dfs)):\n",
    "    dfs[idx] = pd.merge(dfs1[idx], dfs2[idx], on='Unnamed: 0', how='inner')\n",
    "    dfs[idx] = pd.merge(dfs[idx], dfs3[idx], on='Unnamed: 0', how='inner')\n",
    "    \n",
    "    dfs[idx].rename({\n",
    "        'target_age_x': 'target_age_original',\n",
    "        'output_age_x': 'output_age_1',\n",
    "        'age_delta_x': 'age_delta_1',\n",
    "        'loss_x': 'loss_1',\n",
    "        'target_age_y': 'target_age_2',\n",
    "        'output_age_y': 'output_age_2',\n",
    "        'age_delta_y': 'age_delta_2',\n",
    "        'loss_y': 'loss_2',\n",
    "        'target_age': 'target_age_3',\n",
    "        'output_age': 'output_age_3',\n",
    "        'age_delta': 'age_delta_3',\n",
    "        'loss': 'loss_3',\n",
    "    }, axis=1, inplace=True)\n",
    "    \n",
    "    dfs[idx].drop(['target_age_2', 'target_age_3'], axis=1, inplace=True)\n",
    "    \n",
    "    \n",
    "    dfs[idx] = pd.merge(dfs[idx], dfs4[idx], on='Unnamed: 0', how='inner')\n",
    "    dfs[idx].rename({\n",
    "        'output_age': 'output_age_4',\n",
    "        'age_delta': 'age_delta_4',\n",
    "        'loss': 'loss_4',\n",
    "    }, axis=1, inplace=True)\n",
    "    dfs[idx] = dfs[idx].drop(['target_age'], axis=1)\n",
    "    \n",
    "    \n",
    "    dfs[idx] = pd.merge(dfs[idx], dfs5[idx], on='Unnamed: 0', how='inner')\n",
    "    dfs[idx].rename({\n",
    "        'output_age': 'output_age_5',\n",
    "        'age_delta': 'age_delta_5',\n",
    "        'loss': 'loss_5',\n",
    "    }, axis=1, inplace=True)\n",
    "    dfs[idx] = dfs[idx].drop(['target_age'], axis=1)\n",
    "    \n",
    "    dfs[idx] = pd.merge(dfs[idx], dfs6[idx], on='Unnamed: 0', how='inner')\n",
    "    dfs[idx].rename({\n",
    "        'output_age': 'output_age_6',\n",
    "        'age_delta': 'age_delta_6',\n",
    "        'loss': 'loss_6',\n",
    "    }, axis=1, inplace=True)\n",
    "    dfs[idx] = dfs[idx].drop(['target_age'], axis=1)\n",
    "      \n",
    "    \n",
    "#     dfs[idx] = pd.merge(dfs[idx], dfs7[idx], on='Unnamed: 0', how='inner')\n",
    "#     dfs[idx].rename({\n",
    "#         'output_age': 'output_age_7',\n",
    "#         'age_delta': 'age_delta_7',\n",
    "#         'loss': 'loss_7',\n",
    "#     }, axis=1, inplace=True)\n",
    "#     dfs[idx] = dfs[idx].drop(['target_age'], axis=1)\n",
    "    \n",
    "#     dfs[idx] = pd.merge(dfs[idx], dfs8[idx], on='Unnamed: 0', how='inner')\n",
    "#     dfs[idx].rename({\n",
    "#         'output_age': 'output_age_8',\n",
    "#         'age_delta': 'age_delta_8',\n",
    "#         'loss': 'loss_8',\n",
    "#     }, axis=1, inplace=True)\n",
    "#     dfs[idx] = dfs[idx].drop(['target_age'], axis=1)\n",
    "    \n",
    "#     dfs[idx] = pd.merge(dfs[idx], dfs9[idx], on='Unnamed: 0', how='inner')\n",
    "#     dfs[idx].rename({\n",
    "#         'output_age': 'output_age_9',\n",
    "#         'age_delta': 'age_delta_9',\n",
    "#         'loss': 'loss_9',\n",
    "#     }, axis=1, inplace=True)\n",
    "#     dfs[idx] = dfs[idx].drop(['target_age'], axis=1)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     dfs[idx] = pd.merge(dfs[idx], dfs10[idx], on='Unnamed: 0', how='inner')\n",
    "#     dfs[idx].rename({\n",
    "#         'output_age': 'output_age_10',\n",
    "#         'age_delta': 'age_delta_10',\n",
    "#         'loss': 'loss_10',\n",
    "#     }, axis=1, inplace=True)\n",
    "#     dfs[idx] = dfs[idx].drop(['target_age'], axis=1)\n",
    "    \n",
    "#     dfs[idx] = pd.merge(dfs[idx], dfs11[idx], on='Unnamed: 0', how='inner')\n",
    "#     dfs[idx].rename({\n",
    "#         'output_age': 'output_age_11',\n",
    "#         'age_delta': 'age_delta_11',\n",
    "#         'loss': 'loss_11',\n",
    "#     }, axis=1, inplace=True)\n",
    "#     dfs[idx] = dfs[idx].drop(['target_age'], axis=1)\n",
    "    \n",
    "#     dfs[idx] = pd.merge(dfs[idx], dfs12[idx], on='Unnamed: 0', how='inner')\n",
    "#     dfs[idx].rename({\n",
    "#         'output_age': 'output_age_12',\n",
    "#         'age_delta': 'age_delta_12',\n",
    "#         'loss': 'loss_12',\n",
    "#     }, axis=1, inplace=True)\n",
    "#     dfs[idx] = dfs[idx].drop(['target_age'], axis=1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    dfs[idx].rename({\n",
    "        'target_age_original': 'target_age',\n",
    "    }, axis=1, inplace=True)\n",
    "    \n",
    "#     dfs[idx]['mean_output_age'] = dfs[idx][['output_age_1', 'output_age_2', 'output_age_3',\n",
    "#                                            'output_age_4', 'output_age_5', 'output_age_6',\n",
    "#                                            'output_age_7', 'output_age_8', 'output_age_9',\n",
    "#                                            ]].mean(axis=1)\n",
    "#     dfs[idx]['mean_age_delta'] = dfs[idx]['mean_output_age'] - dfs[idx]['target_age']\n",
    "    \n",
    "for idx in range(len(dfs)):\n",
    "    for idx2 in range(1,7):\n",
    "        key1 = 'output_age_decon_' + str(idx2)\n",
    "        key2 = 'output_age_' + str(idx2)\n",
    "        dfs[idx][key1] = age_debiasing(dfs[idx][key2], dfs[idx].target_age)\n",
    "        key3 = 'age_delta_decon_' + str(idx2)\n",
    "        dfs[idx][key3] = dfs[idx][key1] - dfs[idx].target_age\n",
    "    \n",
    "#     dfs[idx]['output_age_decon'] = age_debiasing(dfs[idx].mean_output_age, dfs[idx].target_age)\n",
    "#     dfs[idx]['age_delta_decon'] = dfs[idx].output_age_decon - dfs[idx].target_age\n",
    "\n",
    "\n",
    "print(\"{:15} : {:7}  |  {:3}  |  {:3}  |  {:3}  |  {:3}  |  {:3}  \".format(\n",
    "    'Mod/Std', '0', \"0.04\", \"0.08\", \"0.12\", '0.16', \"0.2\"))\n",
    "print('-----------------------------------------------------------------------------------------------')\n",
    "\n",
    "for idx in range(len(dfs)):\n",
    "#     _, dfs[idx] = train_test_split(dfs[idx], test_size=0.5, random_state=1)\n",
    "    print(\"{:15} : {:7.3f}  |  {:.3f} |  {:.3f} |  {:.3f}   |  {:.3f} |  {:.3f} |   \".format(\n",
    "        names[idx],\n",
    "        np.abs(dfs[idx].age_delta_1).mean(), \n",
    "        np.abs(dfs[idx].age_delta_2).mean(),\n",
    "        np.abs(dfs[idx].age_delta_3).mean(),\n",
    "        np.abs(dfs[idx].age_delta_4).mean(), \n",
    "        np.abs(dfs[idx].age_delta_5).mean(),\n",
    "        np.abs(dfs[idx].age_delta_6).mean(),\n",
    "#         np.abs(dfs[idx].age_delta_7).mean(), \n",
    "#         np.abs(dfs[idx].age_delta_8).mean(),\n",
    "#         np.abs(dfs[idx].age_delta_9).mean(),\n",
    "#         np.abs(dfs[idx].age_delta_10).mean(), \n",
    "#         np.abs(dfs[idx].age_delta_11).mean(),\n",
    "#         np.abs(dfs[idx].age_delta_12).mean(),\n",
    "    )\n",
    "         )\n",
    "    \n",
    "print('\\n')\n",
    "\n",
    "d = {'modality':names, 'dataframe':dfs}\n",
    "df = pd.DataFrame(data=d)\n",
    "df.to_pickle('female_test_testB_SWIN_RandomBiasField_retrain.pkl', protocol = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_pickle('female_test_testB_CNN_RandomBiasField_retrain.pkl')\n",
    "df2 = pd.read_pickle('female_test_testB_SWIN_RandomBiasField_retrain.pkl')\n",
    "\n",
    "noiseSTD = [\n",
    "    0,0.4,0.8,1.2,1.6,2\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{:15} : {:7}  |  {:3}  |  {:3}  |  {:3}  |  {:3}  |  {:3}  \".format(\n",
    "    'Mod/Std', '0', \"0.04\", \"0.08\", \"0.12\", '0.16', \"0.2\"))\n",
    "print('-----------------------------------------------------------------------------------------------')\n",
    "\n",
    "for idx in range(len(dfs)):\n",
    "#     _, dfs[idx] = train_test_split(dfs[idx], test_size=0.5, random_state=1)\n",
    "    print(\"{:15} : {:7.3f}  |  {:.3f} |  {:.3f} |  {:.3f}   |  {:.3f} |  {:.3f} |   \".format(\n",
    "        names[idx],\n",
    "        np.abs(df1.iloc[idx].dataframe.age_delta_1).mean(), \n",
    "        np.abs(df1.iloc[idx].dataframe.age_delta_2).mean(),\n",
    "        np.abs(df1.iloc[idx].dataframe.age_delta_3).mean(),\n",
    "        np.abs(df1.iloc[idx].dataframe.age_delta_4).mean(), \n",
    "        np.abs(df1.iloc[idx].dataframe.age_delta_5).mean(),\n",
    "        np.abs(df1.iloc[idx].dataframe.age_delta_6).mean(),\n",
    "    )\n",
    "         )\n",
    "    \n",
    "print('\\n')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{:15} : {:7}  |  {:3}  |  {:3}  |  {:3}  |  {:3}  |  {:3}  \".format(\n",
    "    'Mod/Std', '0', \"0.04\", \"0.08\", \"0.12\", '0.16', \"0.2\"))\n",
    "print('-----------------------------------------------------------------------------------------------')\n",
    "\n",
    "for idx in range(len(dfs)):\n",
    "#     _, dfs[idx] = train_test_split(dfs[idx], test_size=0.5, random_state=1)\n",
    "    print(\"{:15} : {:7.3f}  |  {:.3f} |  {:.3f} |  {:.3f}   |  {:.3f} |  {:.3f} |   \".format(\n",
    "        names[idx],\n",
    "        np.abs(df2.iloc[idx].dataframe.age_delta_1).mean(), \n",
    "        np.abs(df2.iloc[idx].dataframe.age_delta_2).mean(),\n",
    "        np.abs(df2.iloc[idx].dataframe.age_delta_3).mean(),\n",
    "        np.abs(df2.iloc[idx].dataframe.age_delta_4).mean(), \n",
    "        np.abs(df2.iloc[idx].dataframe.age_delta_5).mean(),\n",
    "        np.abs(df2.iloc[idx].dataframe.age_delta_6).mean(),\n",
    "    )\n",
    "         )\n",
    "    \n",
    "print('\\n')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{:12} : {:4} | {:4} | {:4} | {:4} | {:4} | {:4} |  \".format(\n",
    "    'Mod/Std', '0','0.04',\"0.08\",\"0.12\",'0.16', \"0.2\"))\n",
    "print('-------------------------------------------------------------------------------------------------------')\n",
    "\n",
    "for idx in range(len(df1)):\n",
    "    print(\"{:12} : {:3.2f} | {:.2f} | {:.2f} | {:.2f} | {:.2f} | {:.2f} \".format(\n",
    "        df1.modality[idx],\n",
    "        np.abs(df1.iloc[idx].dataframe.age_delta_1).mean(), \n",
    "        np.abs(df1.iloc[idx].dataframe.age_delta_2).mean(),\n",
    "        np.abs(df1.iloc[idx].dataframe.age_delta_3).mean(),\n",
    "        np.abs(df1.iloc[idx].dataframe.age_delta_4).mean(), \n",
    "        np.abs(df1.iloc[idx].dataframe.age_delta_5).mean(),\n",
    "        np.abs(df1.iloc[idx].dataframe.age_delta_6).mean(),\n",
    "\n",
    "    )\n",
    "         )\n",
    "    \n",
    "print('\\n')   \n",
    "\n",
    "print(\"{:12} : {:4} | {:4} | {:4} | {:4} | {:4} | {:4} |  \".format(\n",
    "    'Mod/Std', '0','0.04',\"0.08\",\"0.12\",'0.16', \"0.2\"))\n",
    "print('-------------------------------------------------------------------------------------------------------')\n",
    "\n",
    "for idx in range(len(df2)):\n",
    "    print(\"{:12} : {:3.2f} | {:.2f} | {:.2f} | {:.2f} | {:.2f} | {:.2f} \".format(\n",
    "        df2.modality[idx],\n",
    "        np.abs(df2.iloc[idx].dataframe.age_delta_1).mean(), \n",
    "        np.abs(df2.iloc[idx].dataframe.age_delta_2).mean(),\n",
    "        np.abs(df2.iloc[idx].dataframe.age_delta_3).mean(),\n",
    "        np.abs(df2.iloc[idx].dataframe.age_delta_4).mean(), \n",
    "        np.abs(df2.iloc[idx].dataframe.age_delta_5).mean(),\n",
    "        np.abs(df2.iloc[idx].dataframe.age_delta_6).mean(),\n",
    "\n",
    "    )\n",
    "         )\n",
    "    \n",
    "print('\\n')   \n",
    "\n",
    "print('================================================================')\n",
    "print('\\n')   \n",
    "\n",
    "print(\"{:12} : {:4} | {:4} | {:4} | {:4} | {:4} | {:4} |  \".format(\n",
    "    'Mod/Std', '0','0.04',\"0.08\",\"0.12\",'0.16', \"0.2\"))\n",
    "print('-------------------------------------------------------------------------------------------------------')\n",
    "\n",
    "for idx in range(len(df1)):\n",
    "    print(\"{:12} : {:3.2f} | {:.2f} | {:.2f} | {:.2f} | {:.2f} | {:.2f} \".format(\n",
    "        df1.modality[idx],\n",
    "        np.abs(df1.iloc[idx].dataframe.age_delta_decon_1).mean(), \n",
    "        np.abs(df1.iloc[idx].dataframe.age_delta_decon_2).mean(),\n",
    "        np.abs(df1.iloc[idx].dataframe.age_delta_decon_3).mean(),\n",
    "        np.abs(df1.iloc[idx].dataframe.age_delta_decon_4).mean(), \n",
    "        np.abs(df1.iloc[idx].dataframe.age_delta_decon_5).mean(),\n",
    "        np.abs(df1.iloc[idx].dataframe.age_delta_decon_6).mean(),\n",
    "\n",
    "    )\n",
    "         )\n",
    "    \n",
    "print('\\n')   \n",
    "\n",
    "print(\"{:12} : {:4} | {:4} | {:4} | {:4} | {:4} | {:4} |  \".format(\n",
    "    'Mod/Std', '0','0.04',\"0.08\",\"0.12\",'0.16', \"0.2\"))\n",
    "print('-------------------------------------------------------------------------------------------------------')\n",
    "\n",
    "for idx in range(len(df2)):\n",
    "    print(\"{:12} : {:3.2f} | {:.2f} | {:.2f} | {:.2f} | {:.2f} | {:.2f} \".format(\n",
    "        df2.modality[idx],\n",
    "        np.abs(df2.iloc[idx].dataframe.age_delta_decon_1).mean(), \n",
    "        np.abs(df2.iloc[idx].dataframe.age_delta_decon_2).mean(),\n",
    "        np.abs(df2.iloc[idx].dataframe.age_delta_decon_3).mean(),\n",
    "        np.abs(df2.iloc[idx].dataframe.age_delta_decon_4).mean(), \n",
    "        np.abs(df2.iloc[idx].dataframe.age_delta_decon_5).mean(),\n",
    "        np.abs(df2.iloc[idx].dataframe.age_delta_decon_6).mean(),\n",
    "\n",
    "    )\n",
    "         )\n",
    "    \n",
    "print('\\n')  \n",
    "\n",
    "print('\\n')   \n",
    "\n",
    "print('================================================================')\n",
    "print('\\n')   \n",
    "\n",
    "\n",
    "print(\"{:12} : {:4} | {:4} | {:4} | {:4} | {:4} | {:4} |  \".format(\n",
    "    'Mod/Std', '0','0.04',\"0.08\",\"0.12\",'0.16', \"0.2\"))\n",
    "print('-------------------------------------------------------------------------------------------------------')\n",
    "\n",
    "for idx in range(len(df1)):\n",
    "    baseCNN = df1.iloc[idx].dataframe.age_delta_decon_1\n",
    "    baseSWIN = df2.iloc[idx].dataframe.age_delta_decon_1\n",
    "    print(\"{:12} : {:3.2f} | {:.2f} | {:.2f} | {:.2f} | {:.2f} | {:.2f} \".format(\n",
    "        df1.modality[idx],\n",
    "        pearsonr(df1.iloc[idx].dataframe.age_delta_decon_1, baseCNN)[0], \n",
    "        pearsonr(df1.iloc[idx].dataframe.age_delta_decon_2, baseCNN)[0],\n",
    "        pearsonr(df1.iloc[idx].dataframe.age_delta_decon_3, baseCNN)[0],\n",
    "        pearsonr(df1.iloc[idx].dataframe.age_delta_decon_4, baseCNN)[0],\n",
    "        pearsonr(df1.iloc[idx].dataframe.age_delta_decon_5, baseCNN)[0],\n",
    "        pearsonr(df1.iloc[idx].dataframe.age_delta_decon_6, baseCNN)[0],\n",
    "\n",
    "    )\n",
    "         )\n",
    "    \n",
    "print('\\n')   \n",
    "\n",
    "print(\"{:12} : {:4} | {:4} | {:4} | {:4} | {:4} | {:4} |  \".format(\n",
    "    'Mod/Std', '0','0.04',\"0.08\",\"0.12\",'0.16', \"0.2\"))\n",
    "print('-------------------------------------------------------------------------------------------------------')\n",
    "\n",
    "for idx in range(len(df2)):\n",
    "    baseCNN = df1.iloc[idx].dataframe.age_delta_decon_1\n",
    "    baseSWIN = df2.iloc[idx].dataframe.age_delta_decon_1\n",
    "    print(\"{:12} : {:3.2f} | {:.2f} | {:.2f} | {:.2f} | {:.2f} | {:.2f} \".format(\n",
    "        df2.modality[idx],\n",
    "        pearsonr(df2.iloc[idx].dataframe.age_delta_decon_1, baseSWIN)[0], \n",
    "        pearsonr(df2.iloc[idx].dataframe.age_delta_decon_2, baseSWIN)[0],\n",
    "        pearsonr(df2.iloc[idx].dataframe.age_delta_decon_3, baseSWIN)[0],\n",
    "        pearsonr(df2.iloc[idx].dataframe.age_delta_decon_4, baseSWIN)[0],\n",
    "        pearsonr(df2.iloc[idx].dataframe.age_delta_decon_5, baseSWIN)[0],\n",
    "        pearsonr(df2.iloc[idx].dataframe.age_delta_decon_6, baseSWIN)[0],\n",
    "\n",
    "    )\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for idx in range(len(df1)):\n",
    "    mod = str(df1.modality[idx])\n",
    "    cnn = np.array([np.abs(df1.iloc[idx].dataframe.age_delta_1).mean(), \n",
    "        np.abs(df1.iloc[idx].dataframe.age_delta_2).mean(),\n",
    "        np.abs(df1.iloc[idx].dataframe.age_delta_3).mean(),\n",
    "        np.abs(df1.iloc[idx].dataframe.age_delta_4).mean(), \n",
    "        np.abs(df1.iloc[idx].dataframe.age_delta_5).mean(),\n",
    "        np.abs(df1.iloc[idx].dataframe.age_delta_6).mean(),\n",
    " \n",
    "                   ])\n",
    "    swin = np.array([np.abs(df2.iloc[idx].dataframe.age_delta_1).mean(), \n",
    "        np.abs(df2.iloc[idx].dataframe.age_delta_2).mean(),\n",
    "        np.abs(df2.iloc[idx].dataframe.age_delta_3).mean(),\n",
    "        np.abs(df2.iloc[idx].dataframe.age_delta_4).mean(), \n",
    "        np.abs(df2.iloc[idx].dataframe.age_delta_5).mean(),\n",
    "        np.abs(df2.iloc[idx].dataframe.age_delta_6).mean(),\n",
    " \n",
    "                    ])\n",
    "#     pearsonr\n",
    "    fig, axs = plt.subplots(1,6,figsize=(18,3), facecolor='white')\n",
    "    axs = axs.flatten()\n",
    "    \n",
    "    axs[0].plot(noiseSTD, cnn)\n",
    "    axs[0].axhline(y=6.253, color='r', linestyle='dashed', label='Mean Age\\nFailure Case', linewidth=2)\n",
    "    axs[0].plot(noiseSTD, swin)\n",
    "    axs[0].set_title('BA MAE')\n",
    "    \n",
    "    axs[1].plot(noiseSTD, cnn/cnn[0])\n",
    "    axs[1].plot(noiseSTD, swin/swin[0])  \n",
    "    axs[1].set_title('BA MAE Norm.')\n",
    "    \n",
    "    cnn = np.array([np.abs(df1.iloc[idx].dataframe.age_delta_decon_1).mean(), \n",
    "        np.abs(df1.iloc[idx].dataframe.age_delta_decon_2).mean(),\n",
    "        np.abs(df1.iloc[idx].dataframe.age_delta_decon_3).mean(),\n",
    "        np.abs(df1.iloc[idx].dataframe.age_delta_decon_4).mean(), \n",
    "        np.abs(df1.iloc[idx].dataframe.age_delta_decon_5).mean(),\n",
    "        np.abs(df1.iloc[idx].dataframe.age_delta_decon_6).mean(),\n",
    " \n",
    "                   ])\n",
    "    swin = np.array([np.abs(df2.iloc[idx].dataframe.age_delta_decon_1).mean(), \n",
    "        np.abs(df2.iloc[idx].dataframe.age_delta_decon_2).mean(),\n",
    "        np.abs(df2.iloc[idx].dataframe.age_delta_decon_3).mean(),\n",
    "        np.abs(df2.iloc[idx].dataframe.age_delta_decon_4).mean(), \n",
    "        np.abs(df2.iloc[idx].dataframe.age_delta_decon_5).mean(),\n",
    "        np.abs(df2.iloc[idx].dataframe.age_delta_decon_6).mean(),\n",
    "  \n",
    "                    ])\n",
    "    \n",
    "    axs[2].plot(noiseSTD, cnn)\n",
    "    axs[2].plot(noiseSTD, swin)\n",
    "    axs[2].set_title('BA MAE Deb.')\n",
    "    \n",
    "    axs[3].plot(noiseSTD, cnn/cnn[0])\n",
    "    axs[3].plot(noiseSTD, swin/swin[0]) \n",
    "    axs[3].set_title('BA MAE Deb. Norm.')\n",
    "    \n",
    "    cnn = np.array([pearsonr(df1.iloc[idx].dataframe.output_age_1, df1.iloc[idx].dataframe.target_age), \n",
    "        pearsonr(df1.iloc[idx].dataframe.output_age_2, df1.iloc[idx].dataframe.target_age),\n",
    "        pearsonr(df1.iloc[idx].dataframe.output_age_3, df1.iloc[idx].dataframe.target_age),\n",
    "        pearsonr(df1.iloc[idx].dataframe.output_age_4, df1.iloc[idx].dataframe.target_age),\n",
    "        pearsonr(df1.iloc[idx].dataframe.output_age_5, df1.iloc[idx].dataframe.target_age),\n",
    "        pearsonr(df1.iloc[idx].dataframe.output_age_6, df1.iloc[idx].dataframe.target_age),\n",
    "\n",
    "                   ])\n",
    "    swin = np.array([pearsonr(df2.iloc[idx].dataframe.output_age_1, df2.iloc[idx].dataframe.target_age), \n",
    "        pearsonr(df2.iloc[idx].dataframe.output_age_2, df2.iloc[idx].dataframe.target_age),\n",
    "        pearsonr(df2.iloc[idx].dataframe.output_age_3, df2.iloc[idx].dataframe.target_age),\n",
    "        pearsonr(df2.iloc[idx].dataframe.output_age_4, df2.iloc[idx].dataframe.target_age),\n",
    "        pearsonr(df2.iloc[idx].dataframe.output_age_5, df2.iloc[idx].dataframe.target_age),\n",
    "        pearsonr(df2.iloc[idx].dataframe.output_age_6, df2.iloc[idx].dataframe.target_age),\n",
    "\n",
    "                   ])\n",
    "    \n",
    "#     pearsonr(dfs[idx].target_age, dfs[idx].mean_output_age)[0]\n",
    "    \n",
    "    axs[4].plot(noiseSTD, cnn[:,0])\n",
    "    axs[4].plot(noiseSTD, swin[:,0]) \n",
    "    axs[4].set_title('Corr(R): Pred v. True Age')\n",
    "    \n",
    "    baseCNN = df1.iloc[idx].dataframe.age_delta_decon_1\n",
    "    baseSWIN = df2.iloc[idx].dataframe.age_delta_decon_1\n",
    "    \n",
    "    cnn = np.array([pearsonr(df1.iloc[idx].dataframe.age_delta_decon_1, baseCNN), \n",
    "        pearsonr(df1.iloc[idx].dataframe.age_delta_decon_2, baseCNN),\n",
    "        pearsonr(df1.iloc[idx].dataframe.age_delta_decon_3, baseCNN),\n",
    "        pearsonr(df1.iloc[idx].dataframe.age_delta_decon_4, baseCNN),\n",
    "        pearsonr(df1.iloc[idx].dataframe.age_delta_decon_5, baseCNN),\n",
    "        pearsonr(df1.iloc[idx].dataframe.age_delta_decon_6, baseCNN),\n",
    "\n",
    "                   ])\n",
    "    swin = np.array([pearsonr(df2.iloc[idx].dataframe.age_delta_decon_1, baseSWIN), \n",
    "        pearsonr(df2.iloc[idx].dataframe.age_delta_decon_2, baseSWIN),\n",
    "        pearsonr(df2.iloc[idx].dataframe.age_delta_decon_3, baseSWIN),\n",
    "        pearsonr(df2.iloc[idx].dataframe.age_delta_decon_4, baseSWIN),\n",
    "        pearsonr(df2.iloc[idx].dataframe.age_delta_decon_5, baseSWIN),\n",
    "        pearsonr(df2.iloc[idx].dataframe.age_delta_decon_6, baseSWIN),\n",
    "\n",
    "                   ])\n",
    "    \n",
    "    \n",
    "    axs[5].plot(noiseSTD, cnn[:,0])\n",
    "    axs[5].plot(noiseSTD, swin[:,0])  \n",
    "    axs[5].set_title('Corr BAD Deb vs. 0-Noise')\n",
    "    \n",
    "    fig.suptitle(mod + ' | Blue=CNN, Orange=SWIN', y=1.05)\n",
    "    fig.supxlabel('Random Bias Field Coefficients', y=-0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for idx in range(len(df1)):\n",
    "    \n",
    "    mod = df1.modality[idx]\n",
    "    \n",
    "    fig, axs = plt.subplots(1,2,figsize=(6,3))\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    baseCNN = df1.iloc[idx].dataframe.age_delta_1\n",
    "    baseSWIN = df2.iloc[idx].dataframe.age_delta_1\n",
    "    \n",
    "    cnn = np.array([pearsonr(df1.iloc[idx].dataframe.age_delta_1, baseCNN), \n",
    "        pearsonr(df1.iloc[idx].dataframe.age_delta_2, baseCNN),\n",
    "        pearsonr(df1.iloc[idx].dataframe.age_delta_3, baseCNN),\n",
    "        pearsonr(df1.iloc[idx].dataframe.age_delta_4, baseCNN),\n",
    "        pearsonr(df1.iloc[idx].dataframe.age_delta_5, baseCNN),\n",
    "        pearsonr(df1.iloc[idx].dataframe.age_delta_6, baseCNN),\n",
    "\n",
    "                   ])\n",
    "    swin = np.array([pearsonr(df2.iloc[idx].dataframe.age_delta_1, baseSWIN), \n",
    "        pearsonr(df2.iloc[idx].dataframe.age_delta_2, baseSWIN),\n",
    "        pearsonr(df2.iloc[idx].dataframe.age_delta_3, baseSWIN),\n",
    "        pearsonr(df2.iloc[idx].dataframe.age_delta_4, baseSWIN),\n",
    "        pearsonr(df2.iloc[idx].dataframe.age_delta_5, baseSWIN),\n",
    "        pearsonr(df2.iloc[idx].dataframe.age_delta_6, baseSWIN),\n",
    "   \n",
    "                   ])\n",
    "    \n",
    "    axs[0].plot(noiseSTD, cnn[:,0])\n",
    "    axs[0].plot(noiseSTD, swin[:,0])\n",
    "    axs[0].set_title('Corr BAD vs. 0-Noise')\n",
    "    \n",
    "    \n",
    "    baseCNN = df1.iloc[idx].dataframe.age_delta_decon_1\n",
    "    baseSWIN = df2.iloc[idx].dataframe.age_delta_decon_1\n",
    "    \n",
    "    cnn = np.array([pearsonr(df1.iloc[idx].dataframe.age_delta_decon_1, baseCNN), \n",
    "        pearsonr(df1.iloc[idx].dataframe.age_delta_decon_2, baseCNN),\n",
    "        pearsonr(df1.iloc[idx].dataframe.age_delta_decon_3, baseCNN),\n",
    "        pearsonr(df1.iloc[idx].dataframe.age_delta_decon_4, baseCNN),\n",
    "        pearsonr(df1.iloc[idx].dataframe.age_delta_decon_5, baseCNN),\n",
    "        pearsonr(df1.iloc[idx].dataframe.age_delta_decon_6, baseCNN),\n",
    "      \n",
    "                   ])\n",
    "    swin = np.array([pearsonr(df2.iloc[idx].dataframe.age_delta_decon_1, baseSWIN), \n",
    "        pearsonr(df2.iloc[idx].dataframe.age_delta_decon_2, baseSWIN),\n",
    "        pearsonr(df2.iloc[idx].dataframe.age_delta_decon_3, baseSWIN),\n",
    "        pearsonr(df2.iloc[idx].dataframe.age_delta_decon_4, baseSWIN),\n",
    "        pearsonr(df2.iloc[idx].dataframe.age_delta_decon_5, baseSWIN),\n",
    "        pearsonr(df2.iloc[idx].dataframe.age_delta_decon_6, baseSWIN),\n",
    "\n",
    "                   ])\n",
    "    \n",
    "    \n",
    "    axs[1].plot(noiseSTD, cnn[:,0])\n",
    "    axs[1].plot(noiseSTD, swin[:,0])  \n",
    "    axs[1].set_title('Corr BAD Deb vs. 0-Noise')\n",
    "    \n",
    "    fig.suptitle(mod, y=1.05)\n",
    "    fig.supxlabel('Noise std', y=-0.06)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_pickle('female_test_testB_CNN_RandomBiasField.pkl')\n",
    "df2 = pd.read_pickle('female_test_testB_SWIN_RandomBiasField.pkl')\n",
    "\n",
    "noiseSTD = [0, .2, .4, .6, .8, 1, 1.2, 1.4, 1.6, 1.8, 2]\n",
    "\n",
    "\n",
    "df1 = pd.concat([df1, pd.read_pickle('female_test_testB_CNN_RandomBiasField_retrain.pkl')])\n",
    "df2 = pd.concat([df2,  pd.read_pickle('female_test_testB_SWIN_RandomBiasField_retrain.pkl')])\n",
    "\n",
    "noiseSTDr = [\n",
    "    0,0.4,0.8,1.2,1.6,2\n",
    "]\n",
    "\n",
    "fig, axs = plt.subplots(2,6,figsize=(21,8.6), facecolor='white')\n",
    "axs = axs.flatten()\n",
    "names = ['Pre-Trained', 'Retrained']\n",
    "\n",
    "axsIdx = 0\n",
    "\n",
    "title_font_size=22\n",
    "ticks_font_size=18\n",
    "legend_font = 18\n",
    "\n",
    "\n",
    "for idx in range(len(df1)):\n",
    "#     mod = str(df1.modality[idx])\n",
    "    \n",
    "    if idx != 1:\n",
    "    \n",
    "        cnn = np.array([np.abs(df1.iloc[idx].dataframe.age_delta_1).mean(), \n",
    "            np.abs(df1.iloc[idx].dataframe.age_delta_2).mean(),\n",
    "            np.abs(df1.iloc[idx].dataframe.age_delta_3).mean(),\n",
    "            np.abs(df1.iloc[idx].dataframe.age_delta_4).mean(), \n",
    "            np.abs(df1.iloc[idx].dataframe.age_delta_5).mean(),\n",
    "            np.abs(df1.iloc[idx].dataframe.age_delta_6).mean(),\n",
    "            np.abs(df1.iloc[idx].dataframe.age_delta_7).mean(), \n",
    "            np.abs(df1.iloc[idx].dataframe.age_delta_8).mean(),\n",
    "            np.abs(df1.iloc[idx].dataframe.age_delta_9).mean(),\n",
    "            np.abs(df1.iloc[idx].dataframe.age_delta_10).mean(), \n",
    "            np.abs(df1.iloc[idx].dataframe.age_delta_11).mean(),\n",
    "#             np.abs(df1.iloc[idx].dataframe.age_delta_12).mean()\n",
    "                       ])\n",
    "        swin = np.array([np.abs(df2.iloc[idx].dataframe.age_delta_1).mean(), \n",
    "            np.abs(df2.iloc[idx].dataframe.age_delta_2).mean(),\n",
    "            np.abs(df2.iloc[idx].dataframe.age_delta_3).mean(),\n",
    "            np.abs(df2.iloc[idx].dataframe.age_delta_4).mean(), \n",
    "            np.abs(df2.iloc[idx].dataframe.age_delta_5).mean(),\n",
    "            np.abs(df2.iloc[idx].dataframe.age_delta_6).mean(),\n",
    "            np.abs(df2.iloc[idx].dataframe.age_delta_7).mean(), \n",
    "            np.abs(df2.iloc[idx].dataframe.age_delta_8).mean(),\n",
    "            np.abs(df2.iloc[idx].dataframe.age_delta_9).mean(),\n",
    "            np.abs(df2.iloc[idx].dataframe.age_delta_10).mean(), \n",
    "            np.abs(df2.iloc[idx].dataframe.age_delta_11).mean(),\n",
    "#             np.abs(df2.iloc[idx].dataframe.age_delta_12).mean()\n",
    "                        ])\n",
    "        \n",
    "        axs[axsIdx].plot(noiseSTD, cnn)\n",
    "        axs[axsIdx].plot(noiseSTD, swin)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        cnn = np.array([np.abs(df1.iloc[idx].dataframe.age_delta_1).mean(), \n",
    "            np.abs(df1.iloc[idx].dataframe.age_delta_2).mean(),\n",
    "            np.abs(df1.iloc[idx].dataframe.age_delta_3).mean(),\n",
    "            np.abs(df1.iloc[idx].dataframe.age_delta_4).mean(), \n",
    "            np.abs(df1.iloc[idx].dataframe.age_delta_5).mean(),\n",
    "            np.abs(df1.iloc[idx].dataframe.age_delta_6).mean()])\n",
    "        swin = np.array([np.abs(df2.iloc[idx].dataframe.age_delta_1).mean(), \n",
    "            np.abs(df2.iloc[idx].dataframe.age_delta_2).mean(),\n",
    "            np.abs(df2.iloc[idx].dataframe.age_delta_3).mean(),\n",
    "            np.abs(df2.iloc[idx].dataframe.age_delta_4).mean(), \n",
    "            np.abs(df2.iloc[idx].dataframe.age_delta_5).mean(),\n",
    "            np.abs(df2.iloc[idx].dataframe.age_delta_6).mean()])\n",
    "        \n",
    "        axs[axsIdx].plot(noiseSTDr, cnn)\n",
    "        axs[axsIdx].plot(noiseSTDr, swin)\n",
    "\n",
    "    \n",
    "    \n",
    "    if idx==1:\n",
    "        axs[axsIdx].axhline(y=6.253, color='r', linestyle='dashed', label='Mean Age Failure Case', linewidth=2)\n",
    "    else:\n",
    "        axs[axsIdx].axhline(y=6.253, color='r', linestyle='dashed', linewidth=2)\n",
    "    \n",
    "    factor = 0.75\n",
    "    axs[axsIdx].set_ylabel(names[idx], fontsize=title_font_size*factor)\n",
    "    \n",
    "    if idx != 1:\n",
    "        axs[axsIdx+1].plot(noiseSTD, cnn/cnn[0])\n",
    "        axs[axsIdx+1].plot(noiseSTD, swin/swin[0])  \n",
    "    else:\n",
    "        axs[axsIdx+1].plot(noiseSTDr, cnn/cnn[0])\n",
    "        axs[axsIdx+1].plot(noiseSTDr, swin/swin[0])  \n",
    "    \n",
    "    if idx != 1 :\n",
    "        cnn = np.array([np.abs(df1.iloc[idx].dataframe.age_delta_decon_1).mean(), \n",
    "            np.abs(df1.iloc[idx].dataframe.age_delta_decon_2).mean(),\n",
    "            np.abs(df1.iloc[idx].dataframe.age_delta_decon_3).mean(),\n",
    "            np.abs(df1.iloc[idx].dataframe.age_delta_decon_4).mean(), \n",
    "            np.abs(df1.iloc[idx].dataframe.age_delta_decon_5).mean(),\n",
    "            np.abs(df1.iloc[idx].dataframe.age_delta_decon_6).mean(),\n",
    "            np.abs(df1.iloc[idx].dataframe.age_delta_decon_7).mean(), \n",
    "            np.abs(df1.iloc[idx].dataframe.age_delta_decon_8).mean(),\n",
    "            np.abs(df1.iloc[idx].dataframe.age_delta_decon_9).mean(),\n",
    "            np.abs(df1.iloc[idx].dataframe.age_delta_decon_10).mean(), \n",
    "            np.abs(df1.iloc[idx].dataframe.age_delta_decon_11).mean(),\n",
    "#             np.abs(df1.iloc[idx].dataframe.age_delta_decon_12).mean()\n",
    "                       ])\n",
    "        swin = np.array([np.abs(df2.iloc[idx].dataframe.age_delta_decon_1).mean(), \n",
    "            np.abs(df2.iloc[idx].dataframe.age_delta_decon_2).mean(),\n",
    "            np.abs(df2.iloc[idx].dataframe.age_delta_decon_3).mean(),\n",
    "            np.abs(df2.iloc[idx].dataframe.age_delta_decon_4).mean(), \n",
    "            np.abs(df2.iloc[idx].dataframe.age_delta_decon_5).mean(),\n",
    "            np.abs(df2.iloc[idx].dataframe.age_delta_decon_6).mean(),\n",
    "            np.abs(df2.iloc[idx].dataframe.age_delta_decon_7).mean(), \n",
    "            np.abs(df2.iloc[idx].dataframe.age_delta_decon_8).mean(),\n",
    "            np.abs(df2.iloc[idx].dataframe.age_delta_decon_9).mean(),\n",
    "            np.abs(df2.iloc[idx].dataframe.age_delta_decon_10).mean(), \n",
    "            np.abs(df2.iloc[idx].dataframe.age_delta_decon_11).mean(),\n",
    "#             np.abs(df2.iloc[idx].dataframe.age_delta_decon_12).mean()\n",
    "                        ])\n",
    "        \n",
    "        axs[axsIdx+2].plot(noiseSTD, cnn)\n",
    "        axs[axsIdx+2].plot(noiseSTD, swin)\n",
    "\n",
    "\n",
    "        axs[axsIdx+3].plot(noiseSTD, cnn/cnn[0])\n",
    "        axs[axsIdx+3].plot(noiseSTD, swin/swin[0]) \n",
    "    else:\n",
    "        cnn = np.array([np.abs(df1.iloc[idx].dataframe.age_delta_decon_1).mean(), \n",
    "            np.abs(df1.iloc[idx].dataframe.age_delta_decon_2).mean(),\n",
    "            np.abs(df1.iloc[idx].dataframe.age_delta_decon_3).mean(),\n",
    "            np.abs(df1.iloc[idx].dataframe.age_delta_decon_4).mean(), \n",
    "            np.abs(df1.iloc[idx].dataframe.age_delta_decon_5).mean(),\n",
    "            np.abs(df1.iloc[idx].dataframe.age_delta_decon_6).mean()])\n",
    "        swin = np.array([np.abs(df2.iloc[idx].dataframe.age_delta_decon_1).mean(), \n",
    "            np.abs(df2.iloc[idx].dataframe.age_delta_decon_2).mean(),\n",
    "            np.abs(df2.iloc[idx].dataframe.age_delta_decon_3).mean(),\n",
    "            np.abs(df2.iloc[idx].dataframe.age_delta_decon_4).mean(), \n",
    "            np.abs(df2.iloc[idx].dataframe.age_delta_decon_5).mean(),\n",
    "            np.abs(df2.iloc[idx].dataframe.age_delta_decon_6).mean()])\n",
    "        \n",
    "        axs[axsIdx+2].plot(noiseSTDr, cnn)\n",
    "        axs[axsIdx+2].plot(noiseSTDr, swin)\n",
    "\n",
    "\n",
    "        axs[axsIdx+3].plot(noiseSTDr, cnn/cnn[0])\n",
    "        axs[axsIdx+3].plot(noiseSTDr, swin/swin[0]) \n",
    "    \n",
    "    \n",
    "    \n",
    "    if idx != 1:\n",
    "        cnn = np.array([pearsonr(df1.iloc[idx].dataframe.output_age_1, df1.iloc[idx].dataframe.target_age), \n",
    "            pearsonr(df1.iloc[idx].dataframe.output_age_2, df1.iloc[idx].dataframe.target_age),\n",
    "            pearsonr(df1.iloc[idx].dataframe.output_age_3, df1.iloc[idx].dataframe.target_age),\n",
    "            pearsonr(df1.iloc[idx].dataframe.output_age_4, df1.iloc[idx].dataframe.target_age),\n",
    "            pearsonr(df1.iloc[idx].dataframe.output_age_5, df1.iloc[idx].dataframe.target_age),\n",
    "            pearsonr(df1.iloc[idx].dataframe.output_age_6, df1.iloc[idx].dataframe.target_age),\n",
    "            pearsonr(df1.iloc[idx].dataframe.output_age_7, df1.iloc[idx].dataframe.target_age),\n",
    "            pearsonr(df1.iloc[idx].dataframe.output_age_8, df1.iloc[idx].dataframe.target_age),\n",
    "            pearsonr(df1.iloc[idx].dataframe.output_age_9, df1.iloc[idx].dataframe.target_age),\n",
    "            pearsonr(df1.iloc[idx].dataframe.output_age_10, df1.iloc[idx].dataframe.target_age),\n",
    "            pearsonr(df1.iloc[idx].dataframe.output_age_11, df1.iloc[idx].dataframe.target_age),\n",
    "#             pearsonr(df1.iloc[idx].dataframe.output_age_12, df1.iloc[idx].dataframe.target_age),\n",
    "                       ])\n",
    "        swin = np.array([pearsonr(df2.iloc[idx].dataframe.output_age_1, df2.iloc[idx].dataframe.target_age), \n",
    "            pearsonr(df2.iloc[idx].dataframe.output_age_2, df2.iloc[idx].dataframe.target_age),\n",
    "            pearsonr(df2.iloc[idx].dataframe.output_age_3, df2.iloc[idx].dataframe.target_age),\n",
    "            pearsonr(df2.iloc[idx].dataframe.output_age_4, df2.iloc[idx].dataframe.target_age),\n",
    "            pearsonr(df2.iloc[idx].dataframe.output_age_5, df2.iloc[idx].dataframe.target_age),\n",
    "            pearsonr(df2.iloc[idx].dataframe.output_age_6, df2.iloc[idx].dataframe.target_age),\n",
    "            pearsonr(df2.iloc[idx].dataframe.output_age_7, df2.iloc[idx].dataframe.target_age),\n",
    "            pearsonr(df2.iloc[idx].dataframe.output_age_8, df2.iloc[idx].dataframe.target_age),\n",
    "            pearsonr(df2.iloc[idx].dataframe.output_age_9, df2.iloc[idx].dataframe.target_age),\n",
    "            pearsonr(df2.iloc[idx].dataframe.output_age_10, df2.iloc[idx].dataframe.target_age),\n",
    "            pearsonr(df2.iloc[idx].dataframe.output_age_11, df2.iloc[idx].dataframe.target_age),\n",
    "#             pearsonr(df2.iloc[idx].dataframe.output_age_12, df2.iloc[idx].dataframe.target_age),\n",
    "                       ])\n",
    "        \n",
    "        axs[axsIdx+4].plot(noiseSTD, cnn[:,0])\n",
    "        axs[axsIdx+4].plot(noiseSTD, swin[:,0]) \n",
    "        \n",
    "    else:\n",
    "        cnn = np.array([pearsonr(df1.iloc[idx].dataframe.output_age_1, df1.iloc[idx].dataframe.target_age), \n",
    "            pearsonr(df1.iloc[idx].dataframe.output_age_2, df1.iloc[idx].dataframe.target_age),\n",
    "            pearsonr(df1.iloc[idx].dataframe.output_age_3, df1.iloc[idx].dataframe.target_age),\n",
    "            pearsonr(df1.iloc[idx].dataframe.output_age_4, df1.iloc[idx].dataframe.target_age),\n",
    "            pearsonr(df1.iloc[idx].dataframe.output_age_5, df1.iloc[idx].dataframe.target_age),\n",
    "            pearsonr(df1.iloc[idx].dataframe.output_age_6, df1.iloc[idx].dataframe.target_age),\n",
    "\n",
    "                       ])\n",
    "        swin = np.array([pearsonr(df2.iloc[idx].dataframe.output_age_1, df2.iloc[idx].dataframe.target_age), \n",
    "            pearsonr(df2.iloc[idx].dataframe.output_age_2, df2.iloc[idx].dataframe.target_age),\n",
    "            pearsonr(df2.iloc[idx].dataframe.output_age_3, df2.iloc[idx].dataframe.target_age),\n",
    "            pearsonr(df2.iloc[idx].dataframe.output_age_4, df2.iloc[idx].dataframe.target_age),\n",
    "            pearsonr(df2.iloc[idx].dataframe.output_age_5, df2.iloc[idx].dataframe.target_age),\n",
    "            pearsonr(df2.iloc[idx].dataframe.output_age_6, df2.iloc[idx].dataframe.target_age),\n",
    "\n",
    "                   ])\n",
    "        axs[axsIdx+4].plot(noiseSTDr, cnn[:,0])\n",
    "        axs[axsIdx+4].plot(noiseSTDr, swin[:,0]) \n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    baseCNN = df1.iloc[idx].dataframe.age_delta_decon_1\n",
    "    baseSWIN = df2.iloc[idx].dataframe.age_delta_decon_1\n",
    "    \n",
    "    if idx !=1:\n",
    "        cnn = np.array([pearsonr(df1.iloc[idx].dataframe.age_delta_decon_1, baseCNN), \n",
    "            pearsonr(df1.iloc[idx].dataframe.age_delta_decon_2, baseCNN),\n",
    "            pearsonr(df1.iloc[idx].dataframe.age_delta_decon_3, baseCNN),\n",
    "            pearsonr(df1.iloc[idx].dataframe.age_delta_decon_4, baseCNN),\n",
    "            pearsonr(df1.iloc[idx].dataframe.age_delta_decon_5, baseCNN),\n",
    "            pearsonr(df1.iloc[idx].dataframe.age_delta_decon_6, baseCNN),\n",
    "            pearsonr(df1.iloc[idx].dataframe.age_delta_decon_7, baseCNN),\n",
    "            pearsonr(df1.iloc[idx].dataframe.age_delta_decon_8, baseCNN),\n",
    "            pearsonr(df1.iloc[idx].dataframe.age_delta_decon_9, baseCNN),\n",
    "            pearsonr(df1.iloc[idx].dataframe.age_delta_decon_10, baseCNN),\n",
    "            pearsonr(df1.iloc[idx].dataframe.age_delta_decon_11, baseCNN),\n",
    "#             pearsonr(df1.iloc[idx].dataframe.age_delta_decon_12, baseCNN),\n",
    "                       ])\n",
    "        swin = np.array([pearsonr(df2.iloc[idx].dataframe.age_delta_decon_1, baseSWIN), \n",
    "            pearsonr(df2.iloc[idx].dataframe.age_delta_decon_2, baseSWIN),\n",
    "            pearsonr(df2.iloc[idx].dataframe.age_delta_decon_3, baseSWIN),\n",
    "            pearsonr(df2.iloc[idx].dataframe.age_delta_decon_4, baseSWIN),\n",
    "            pearsonr(df2.iloc[idx].dataframe.age_delta_decon_5, baseSWIN),\n",
    "            pearsonr(df2.iloc[idx].dataframe.age_delta_decon_6, baseSWIN),\n",
    "            pearsonr(df2.iloc[idx].dataframe.age_delta_decon_7, baseSWIN),\n",
    "            pearsonr(df2.iloc[idx].dataframe.age_delta_decon_8, baseSWIN),\n",
    "            pearsonr(df2.iloc[idx].dataframe.age_delta_decon_9, baseSWIN),\n",
    "            pearsonr(df2.iloc[idx].dataframe.age_delta_decon_10, baseSWIN),\n",
    "            pearsonr(df2.iloc[idx].dataframe.age_delta_decon_11, baseSWIN),\n",
    "#             pearsonr(df2.iloc[idx].dataframe.age_delta_decon_12, baseSWIN),\n",
    "                       ])\n",
    "    else:\n",
    "        cnn = np.array([pearsonr(df1.iloc[idx].dataframe.age_delta_decon_1, baseCNN), \n",
    "            pearsonr(df1.iloc[idx].dataframe.age_delta_decon_2, baseCNN),\n",
    "            pearsonr(df1.iloc[idx].dataframe.age_delta_decon_3, baseCNN),\n",
    "            pearsonr(df1.iloc[idx].dataframe.age_delta_decon_4, baseCNN),\n",
    "            pearsonr(df1.iloc[idx].dataframe.age_delta_decon_5, baseCNN),\n",
    "            pearsonr(df1.iloc[idx].dataframe.age_delta_decon_6, baseCNN),\n",
    "\n",
    "                       ])\n",
    "        swin = np.array([pearsonr(df2.iloc[idx].dataframe.age_delta_decon_1, baseSWIN), \n",
    "            pearsonr(df2.iloc[idx].dataframe.age_delta_decon_2, baseSWIN),\n",
    "            pearsonr(df2.iloc[idx].dataframe.age_delta_decon_3, baseSWIN),\n",
    "            pearsonr(df2.iloc[idx].dataframe.age_delta_decon_4, baseSWIN),\n",
    "            pearsonr(df2.iloc[idx].dataframe.age_delta_decon_5, baseSWIN),\n",
    "            pearsonr(df2.iloc[idx].dataframe.age_delta_decon_6, baseSWIN),\n",
    "\n",
    "                       ])\n",
    "    \n",
    "    if idx == 1:\n",
    "        axs[axsIdx+5].plot(noiseSTDr, cnn[:,0], label='CNN')\n",
    "        axs[axsIdx+5].plot(noiseSTDr, swin[:,0], label='SWIN')  \n",
    "    else:\n",
    "        axs[axsIdx+5].plot(noiseSTD, cnn[:,0])\n",
    "        axs[axsIdx+5].plot(noiseSTD, swin[:,0])  \n",
    "    \n",
    "    if idx == 0:\n",
    "        factor = 0.75\n",
    "        axs[axsIdx].set_title('BA MAE', fontsize=title_font_size*factor)\n",
    "        axs[axsIdx+1].set_title('BA MAE Norm.', fontsize=title_font_size*factor)\n",
    "        axs[axsIdx+2].set_title('BA MAE Deb.', fontsize=title_font_size*factor)\n",
    "        axs[axsIdx+3].set_title('BA MAE Deb. Norm.', fontsize=title_font_size*factor)\n",
    "        axs[axsIdx+4].set_title('Corr(r):\\nPred v. True Age', fontsize=title_font_size*factor)\n",
    "        axs[axsIdx+5].set_title('Corr(r): BAD Deb.\\nvs. Baseline', fontsize=title_font_size*factor)\n",
    "        \n",
    "    if idx != 1:\n",
    "        axs[axsIdx].axes.xaxis.set_ticklabels([])\n",
    "        axs[axsIdx+1].axes.xaxis.set_ticklabels([])\n",
    "        axs[axsIdx+2].axes.xaxis.set_ticklabels([])\n",
    "        axs[axsIdx+3].axes.xaxis.set_ticklabels([])\n",
    "        axs[axsIdx+4].axes.xaxis.set_ticklabels([])\n",
    "        axs[axsIdx+5].axes.xaxis.set_ticklabels([])\n",
    "        \n",
    "    factor = 0.74\n",
    "        \n",
    "    axs[axsIdx].tick_params(axis='both', labelsize=ticks_font_size*factor)\n",
    "    axs[axsIdx+1].tick_params(axis='both', labelsize=ticks_font_size*factor)\n",
    "    axs[axsIdx+2].tick_params(axis='both', labelsize=ticks_font_size*factor)\n",
    "    axs[axsIdx+3].tick_params(axis='both', labelsize=ticks_font_size*factor)\n",
    "    axs[axsIdx+4].tick_params(axis='both', labelsize=ticks_font_size*factor)\n",
    "    axs[axsIdx+5].tick_params(axis='both', labelsize=ticks_font_size*factor)\n",
    "    \n",
    "    axsIdx += 6\n",
    "\n",
    "fig.legend(loc='lower center', ncol=3, bbox_to_anchor=(0.5, -0.05), fontsize=legend_font)\n",
    "fig.supxlabel('Coefficient Factor', y=0.06, fontsize=title_font_size, verticalalignment='top')\n",
    "fig.supylabel('MAE (years) / Pearson Correlation (r)', x= 0.06, fontsize=title_font_size, verticalalignment='center')\n",
    "fig.subplots_adjust(wspace=0.25, hspace=0.05)\n",
    "\n",
    "fig.savefig('CNNvsSWINvBiasFieldT1.png', bbox_inches='tight', facecolor='white')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
