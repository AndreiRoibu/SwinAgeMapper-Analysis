{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d165e76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# qlogin -q short.qg -l gpu=1[affinity=true],gputype=rtx8000\n",
    "# # qlogin -q short.qg -l gpu=1[affinity=true],gputype=a100\n",
    "\n",
    "srun -p gpu_short --gres gpu:1 --constraint='a100-pcie-80gb|quadro-rtx8000' --pty bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e99a803",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "\n",
    "cd /well/win/users/hsv459/SwinAgeMapper\n",
    "\n",
    "module purge\n",
    "module load Python/3.7.4-GCCcore-8.3.0\n",
    "\n",
    "source /well/win-fmrib-analysis/users/hsv459/python/swinagemapper-skylake/bin/activate\n",
    "echo \"Loaded swinagemapper-skylake environment.\"\n",
    "\n",
    "# continue to use your python venv as normal\n",
    "   \n",
    "ipython\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5900c8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import argparse\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "\n",
    "from solver import Solver\n",
    "from utils.data_utils import get_datasets_dynamically, get_test_datasets_dynamically\n",
    "from utils.settings import Settings\n",
    "import utils.data_evaluation as evaluations\n",
    "from utils.misc import create_folder\n",
    "\n",
    "from SwinAgeMapper import SwinAgeMapper\n",
    "\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "\n",
    "MSELoss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9557ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "networks=[\n",
    "    'SN5-1',\n",
    "#     'SN2-9',\n",
    "#     'SN3-3',\n",
    "#     'SN3-8',\n",
    "#     'SN6-10',\n",
    "#     'SN4-12',\n",
    "#     'SN8-13',\n",
    "]\n",
    "\n",
    "transforms = {\n",
    "    'RandomNoise': [0,0.01,0.02,0.04,0.06,0.08,\n",
    "                    0.1,0.12,0.14,0.16,\n",
    "                    0.18,0.2],\n",
    "    'RandomAnisotropy': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], # 1 = no change\n",
    "    'RandomBiasField': [ 1.6, 1.8, 2], #[0, .2, .4, .6, .8, 1, 1.2, 1.4, 1.6, 1.8, 2], # 0 = no change\n",
    "    'RandomMotion': [1,2,3,4,5],\n",
    "    'RandomGhosting': [0, .2, .4, .6, .8, 1, 1.2, 1.4, 1.6, 1.8, 2],# 0 = no change\n",
    "    'RandomSpike': [0, .2, .4, .6, .8, 1, 1.2, 1.4, 1.6, 1.8, 2], # 0 = no change\n",
    "    'RandomGamma': [0, .2, .4, .6, .8, 1, 1.2, 1.4, 1.6, 1.8, 2], # 0 = no change\n",
    "    \n",
    "    'RandomNoiseMask': [0,0.01,0.02,0.04,0.06,0.08,\n",
    "                    0.1,0.12,0.14,0.16,\n",
    "                    0.18,0.2],\n",
    "    'RandomAffine': [0,1,2,3,4,5,6,7,8,9,10]\n",
    "}\n",
    "\n",
    "transformationFlags = ['RandomAffine']\n",
    "\n",
    "for transformationFlag in transformationFlags:\n",
    "    print('======> transformationFlag={}'.format(transformationFlag))\n",
    "    \n",
    "    # transformationFlag = 'RandomAnisotropy'\n",
    "    # transformationFlag = 'RandomBiasField'\n",
    "    transformationMetricS = transforms[transformationFlag]\n",
    "\n",
    "\n",
    "\n",
    "    def load_data_dynamically(data_parameters, mapping_evaluation_parameters=None, flag='train'):\n",
    "\n",
    "        if flag=='train':\n",
    "            print(\"Data is loading...\")\n",
    "            train_data, validation_data, resolution = get_datasets_dynamically(data_parameters)\n",
    "            print(\"Data has loaded!\")\n",
    "            print(\"Training dataset size is {}\".format(len(train_data)))\n",
    "            print(\"Validation dataset size is {}\".format(len(validation_data)))\n",
    "            return train_data, validation_data, resolution\n",
    "        elif flag=='test':\n",
    "            print(\"Data is loading...\")\n",
    "            test_data, volumes_to_be_used, prediction_output_statistics_name, resolution = get_test_datasets_dynamically(data_parameters, mapping_evaluation_parameters)\n",
    "            print(\"Data has loaded!\")\n",
    "            len_test_data = len(test_data)\n",
    "            print(\"Testing dataset size is {}\".format(len_test_data))\n",
    "            return test_data, volumes_to_be_used, prediction_output_statistics_name, len_test_data, resolution\n",
    "        else:\n",
    "            print('ERROR: Invalid Flag')\n",
    "            return None\n",
    "\n",
    "    def _load_mean(dataset_sex):\n",
    "\n",
    "        if dataset_sex == 'male':\n",
    "            mean_age = 64.64810970818492\n",
    "        else:\n",
    "            mean_age = 63.370316719492536\n",
    "\n",
    "        mean_age = np.array(np.float32(mean_age))\n",
    "\n",
    "        return mean_age \n",
    "\n",
    "    def _statistics_calculator(output_age, target_age):\n",
    "\n",
    "        output_age = np.array(output_age)\n",
    "        target_age = np.array(target_age)\n",
    "        age_delta = output_age - target_age\n",
    "        loss = MSELoss(torch.from_numpy(output_age), torch.from_numpy(target_age)).numpy()\n",
    "\n",
    "        return age_delta, loss\n",
    "\n",
    "\n",
    "\n",
    "    for idx in range(len(networks)):\n",
    "\n",
    "\n",
    "        # Find the pretrained network weights\n",
    "\n",
    "        network = networks[idx]\n",
    "\n",
    "        print('NETWORK: {}'.format(network))\n",
    "\n",
    "        network_path = '/well/win-fmrib-analysis/users/hsv459/SwinAgeMapper/saved_models/'\n",
    "        network_name = network + '.pth.tar'\n",
    "        network_path += network_name\n",
    "\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        trained_model = torch.load(network_path, map_location=torch.device(device))\n",
    "\n",
    "\n",
    "        # Load the settings\n",
    "\n",
    "        settings_file_name = network + '.ini'\n",
    "        evaluation_settings_file_name = network + '_eval.ini'\n",
    "        settings = Settings(settings_file_name)\n",
    "        settings_evaluation = Settings(evaluation_settings_file_name)\n",
    "\n",
    "        mapping_evaluation_parameters = settings_evaluation['MAPPING']\n",
    "        data_parameters = settings['DATA']\n",
    "        training_parameters = settings['TRAINING']\n",
    "        network_parameters = settings['NETWORK']\n",
    "        misc_parameters = settings['MISC']\n",
    "\n",
    "        # Get the pretrained model resolution\n",
    "\n",
    "        data_directory = data_parameters['data_directory']\n",
    "        modality_flag = data_parameters['modality_flag']\n",
    "        scaling_values_simple = pd.read_csv(data_parameters['scaling_values'], index_col=0)\n",
    "        resolution = scaling_values_simple.loc[modality_flag].resolution\n",
    "        del scaling_values_simple\n",
    "\n",
    "        # Get the pretrained model ready\n",
    "\n",
    "        model = SwinAgeMapper(\n",
    "            img_size = network_parameters['img_size'],\n",
    "            in_channels = network_parameters['in_channels'],\n",
    "            depths = network_parameters['depths'],\n",
    "            num_heads = network_parameters['num_heads'],\n",
    "            feature_size = network_parameters['feature_size'],\n",
    "            drop_rate = network_parameters['drop_rate'],\n",
    "            attn_drop_rate = network_parameters['attn_drop_rate'],\n",
    "            dropout_path_rate = network_parameters['dropout_path_rate'],\n",
    "            use_checkpoint = network_parameters['use_checkpoint'],\n",
    "            spatial_dims = network_parameters['spatial_dims'],\n",
    "            downsample = network_parameters['downsample'],\n",
    "            # activation= network_parameters['activation'],\n",
    "            fully_connected_activation = network_parameters['fully_connected_activation'],\n",
    "            resolution=resolution,\n",
    "            patch_size=network_parameters['patch_size'],\n",
    "            )\n",
    "\n",
    "        model.load_state_dict(trained_model)\n",
    "\n",
    "        del trained_model, network_path, network_name\n",
    "\n",
    "        if torch.cuda.is_available() == True and device!='cpu':\n",
    "            model.cuda(device)\n",
    "\n",
    "        model.eval();\n",
    "\n",
    "\n",
    "        # Prepare the output folder and several other settings\n",
    "\n",
    "        mapping_evaluation_parameters['experiment_name'] = mapping_evaluation_parameters['experiment_name'] + '_' + transformationFlag\n",
    "        experiment_name = mapping_evaluation_parameters['experiment_name']\n",
    "        prediction_output_path = experiment_name + \"_predictions\"\n",
    "        create_folder(prediction_output_path)\n",
    "\n",
    "\n",
    "        trained_model_path = \"saved_models/\" + experiment_name + \".pth.tar\"\n",
    "        control = mapping_evaluation_parameters['control']\n",
    "        dataset_sex = data_parameters['dataset_sex']\n",
    "\n",
    "        data_parameters['noise_mean'] = 0.\n",
    "\n",
    "\n",
    "        # Start a loop going through the various noise settings\n",
    "\n",
    "        for transformationMetric in transformationMetricS:\n",
    "\n",
    "            print('----> transformationMetric=',transformationMetric)\n",
    "\n",
    "            data_parameters['transformation_flag'] = transformationFlag\n",
    "            data_parameters['transformation_metric'] = transformationMetric\n",
    "            data_parameters['fix_seed'] = False\n",
    "\n",
    "            test_data, volumes_to_be_used, prediction_output_statistics_name, len_test_data, resolution = load_data_dynamically(\n",
    "                                                    data_parameters=data_parameters, \n",
    "                                                    mapping_evaluation_parameters=mapping_evaluation_parameters, \n",
    "                                                    flag='test'\n",
    "                                                    )\n",
    "\n",
    "            test_loader = data.DataLoader(\n",
    "                dataset = test_data,\n",
    "                batch_size=1,\n",
    "                shuffle=False,\n",
    "                pin_memory=True,\n",
    "                num_workers=data_parameters['num_workers']\n",
    "            )\n",
    "\n",
    "            output_statistics = {}\n",
    "            output_statistics_path = os.path.join(prediction_output_path, prediction_output_statistics_name + '_' + str(transformationMetric))\n",
    "\n",
    "            with torch.no_grad():\n",
    "\n",
    "                for batch_index, sampled_batch in enumerate(test_loader):\n",
    "                    X = sampled_batch[0].type(torch.FloatTensor)\n",
    "                    y_age = sampled_batch[1].type(torch.FloatTensor)\n",
    "                    y_age = (y_age.cpu().numpy()).astype('float32')\n",
    "                    y_age = np.squeeze(y_age)\n",
    "                    subject = volumes_to_be_used[batch_index]\n",
    "\n",
    "                    # We add an extra dimension (~ number of channels) for the 3D convolutions.\n",
    "                    if len(X.size())<5:\n",
    "                        X = torch.unsqueeze(X, dim=1)\n",
    "\n",
    "                    if torch.cuda.is_available():\n",
    "                        X = X.cuda(device, non_blocking=True)\n",
    "\n",
    "                    y_hat = model(X)   # Forward pass\n",
    "                    y_hat = (y_hat.cpu().numpy()).astype('float32')\n",
    "                    y_hat = np.squeeze(y_hat)\n",
    "\n",
    "                    target_age = y_age\n",
    "\n",
    "                    age_delta, loss = _statistics_calculator(y_hat, target_age)\n",
    "                    output_statistics[subject] = [target_age, y_hat, age_delta, loss]\n",
    "\n",
    "                    print(\"\\r Processed {:.3f}%: {}/{} subjects\".format((batch_index+1)/len_test_data * 100.0, batch_index+1, len_test_data), end='')\n",
    "\n",
    "                output_statistics_df = pd.DataFrame.from_dict(output_statistics, orient='index', columns=['target_age', 'output_age', 'age_delta', 'loss'])     \n",
    "                output_statistics_df.to_csv(output_statistics_path)\n",
    "\n",
    "            print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ab8a84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1adb90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
