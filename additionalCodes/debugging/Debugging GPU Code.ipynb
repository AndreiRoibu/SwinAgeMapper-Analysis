{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qlogin -q short.qg -l gpu=1[affinity=true],gputype=rtx8000\n",
    "# # qlogin -q short.qg -l gpu=1[affinity=true],gputype=a100\n",
    "\n",
    "srun -p gpu_short --gres gpu:1 --pty bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "\n",
    "cd /well/win/users/hsv459/SwinAgeMapper\n",
    "\n",
    "module purge\n",
    "module load Python/3.7.4-GCCcore-8.3.0\n",
    "\n",
    "CPU_ARCHITECTURE=$(/apps/misc/utils/bin/get-cpu-software-architecture.py)\n",
    "\n",
    "# Error handling\n",
    "if [[ ! $? == 0 ]]; then\n",
    "  echo \"Fatal error: Please send the following information to the BMRC team: Could not determine CPU software architecture on $(hostname)\"\n",
    "  exit 1\n",
    "fi\n",
    "\n",
    "# Activate the ivybridge or skylake version of your python virtual environment\n",
    "source /well/win/users/hsv459/python/swinagemapper-${CPU_ARCHITECTURE}/bin/activate\n",
    "# source /well/win/users/hsv459/python/functionmapper-skylake/bin/activate\n",
    "\n",
    "# continue to use your python venv as normal\n",
    "   \n",
    "# ipython\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import argparse\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "\n",
    "from solver import Solver\n",
    "from utils.data_utils import get_datasets_dynamically, get_test_datasets_dynamically\n",
    "from utils.settings import Settings\n",
    "import utils.data_evaluation as evaluations\n",
    "\n",
    "from SwinAgeMapper import SwinAgeMapper\n",
    "\n",
    "\n",
    "# Set the default floating point tensor type to FloatTensor\n",
    "torch.set_default_tensor_type(torch.FloatTensor)\n",
    "\n",
    "settings_file_name = 'settings.ini'\n",
    "evaluation_settings_file_name = 'settings_evaluation.ini'\n",
    "\n",
    "settings = Settings(settings_file_name)\n",
    "data_parameters = settings['DATA']\n",
    "training_parameters = settings['TRAINING']\n",
    "network_parameters = settings['NETWORK']\n",
    "misc_parameters = settings['MISC']\n",
    "\n",
    "########################################################################################\n",
    "network_parameters['feature_size'] = 12\n",
    "# network_parameters['depths'] = (1, 1, 1, 1)\n",
    "network_parameters['num_heads'] = (2, 4, 8, 16)\n",
    "\n",
    "batch = 5\n",
    "training_parameters['training_batch_size'] = batch\n",
    "training_parameters['validation_batch_size'] = batch\n",
    "########################################################################################\n",
    "\n",
    "\n",
    "def load_data_dynamically(data_parameters, mapping_evaluation_parameters=None, flag='train'):\n",
    "    \n",
    "    if flag=='train':\n",
    "        print(\"Data is loading...\")\n",
    "        train_data, validation_data, resolution = get_datasets_dynamically(data_parameters)\n",
    "        print(\"Data has loaded!\")\n",
    "        print(\"Training dataset size is {}\".format(len(train_data)))\n",
    "        print(\"Validation dataset size is {}\".format(len(validation_data)))\n",
    "        return train_data, validation_data, resolution\n",
    "    elif flag=='test':\n",
    "        print(\"Data is loading...\")\n",
    "        test_data, volumes_to_be_used, prediction_output_statistics_name, resolution = get_test_datasets_dynamically(data_parameters, mapping_evaluation_parameters)\n",
    "        print(\"Data has loaded!\")\n",
    "        len_test_data = len(test_data)\n",
    "        print(\"Testing dataset size is {}\".format(len_test_data))\n",
    "        return test_data, volumes_to_be_used, prediction_output_statistics_name, len_test_data, resolution\n",
    "    else:\n",
    "        print('ERROR: Invalid Flag')\n",
    "        return None\n",
    "    \n",
    "train_data, validation_data, resolution = load_data_dynamically(data_parameters=data_parameters, flag='train')\n",
    "\n",
    "if training_parameters['optimiser'] == 'adamw':\n",
    "    optimizer = torch.optim.AdamW\n",
    "elif training_parameters['optimiser'] == 'adam':\n",
    "    optimizer = torch.optim.Adam\n",
    "else:\n",
    "    optimizer = torch.optim.AdamW # Default option\n",
    "\n",
    "optimizer_arguments={'lr': training_parameters['learning_rate'],\n",
    "                    'betas': training_parameters['optimizer_beta'],\n",
    "                    'eps': training_parameters['optimizer_epsilon'],\n",
    "                    'weight_decay': training_parameters['optimizer_weigth_decay']\n",
    "                    }\n",
    "\n",
    "if training_parameters['loss_function'] == 'mse':\n",
    "    loss_function = torch.nn.MSELoss()\n",
    "elif training_parameters['loss_function'] == 'mae':\n",
    "    loss_function = torch.nn.L1Loss()\n",
    "else:\n",
    "    print(\"Loss function not valid. Defaulting to MSE!\")\n",
    "    loss_function = torch.nn.MSELoss()\n",
    "    \n",
    "train_loader = data.DataLoader(\n",
    "    dataset=train_data,\n",
    "    batch_size=training_parameters['training_batch_size'],\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    num_workers=data_parameters['num_workers']\n",
    ")\n",
    "validation_loader = data.DataLoader(\n",
    "    dataset=validation_data,\n",
    "    batch_size=training_parameters['validation_batch_size'],\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    "    num_workers=data_parameters['num_workers']\n",
    ")\n",
    "\n",
    "AgeMapperModel = SwinAgeMapper(\n",
    "                            img_size = network_parameters['img_size'],\n",
    "                            in_channels = network_parameters['in_channels'],\n",
    "                            depths = network_parameters['depths'],\n",
    "                            num_heads = network_parameters['num_heads'],\n",
    "                            feature_size = network_parameters['feature_size'],\n",
    "                            drop_rate = network_parameters['drop_rate'],\n",
    "                            attn_drop_rate = network_parameters['attn_drop_rate'],\n",
    "                            dropout_path_rate = network_parameters['dropout_path_rate'],\n",
    "                            use_checkpoint = network_parameters['use_checkpoint'],\n",
    "                            spatial_dims = network_parameters['spatial_dims'],\n",
    "                            downsample = network_parameters['downsample'],\n",
    "                            fully_connected_activation = network_parameters['fully_connected_activation'],\n",
    "                            resolution=resolution,\n",
    "                            )\n",
    "\n",
    "solver = Solver(model=AgeMapperModel,\n",
    "                number_of_classes=network_parameters['number_of_classes'],\n",
    "                experiment_name=training_parameters['experiment_name'],\n",
    "                optimizer=optimizer,\n",
    "                optimizer_arguments=optimizer_arguments,\n",
    "                loss_function=loss_function,\n",
    "                model_name=training_parameters['experiment_name'],\n",
    "                number_epochs=training_parameters['number_of_epochs'],\n",
    "                loss_log_period=training_parameters['loss_log_period'],\n",
    "                learning_rate_scheduler_gamma=training_parameters['learning_rate_scheduler_gamma'],\n",
    "                use_last_checkpoint=training_parameters['use_last_checkpoint'],\n",
    "                experiment_directory=misc_parameters['experiments_directory'],\n",
    "                logs_directory=misc_parameters['logs_directory'],\n",
    "                checkpoint_directory=misc_parameters['checkpoint_directory'],\n",
    "                best_checkpoint_directory=misc_parameters['best_checkpoint_directory'],\n",
    "                save_model_directory=misc_parameters['save_model_directory'],\n",
    "                learning_rate_scheduler_flag = training_parameters['learning_rate_scheduler_flag'],\n",
    "                learning_rate_scheduler_patience=training_parameters['learning_rate_scheduler_patience'],\n",
    "                learning_rate_scheduler_threshold=training_parameters['learning_rate_scheduler_threshold'],\n",
    "                learning_rate_scheduler_min_value=training_parameters['learning_rate_scheduler_min_value'],\n",
    "                lr_cosine_scheduler_warmup_epochs = training_parameters['lr_cosine_scheduler_warmup_epochs'],\n",
    "                early_stopping_patience=training_parameters['early_stopping_patience'],\n",
    "                early_stopping_min_patience=training_parameters['early_stopping_min_patience'],\n",
    "                early_stopping_min_delta=training_parameters['early_stopping_min_delta'],\n",
    "                use_pre_trained = training_parameters['use_pre_trained'],\n",
    "                )\n",
    "\n",
    "solver.train(train_loader, validation_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[NETWORK]\n",
    "img_size = (160, 192, 160)\n",
    "patch_size=5\n",
    "in_channels = 1\n",
    "depths = (2, 2, 2, 2)\n",
    "num_heads = (3, 6, 12, 24)\n",
    "feature_size = 48\n",
    "drop_rate = 0.0\n",
    "attn_drop_rate = 0.0\n",
    "dropout_path_rate = 0.0\n",
    "use_checkpoint = True\n",
    "spatial_dims = 3\n",
    "downsample=\"merging\"\n",
    "fully_connected_activation=\"relu\"\n",
    "number_of_classes = 1\n",
    "\n",
    "[TRAINING]\n",
    "; ---> Model Properties\n",
    "experiment_name = \"SM0-55\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import argparse\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "\n",
    "from solver import Solver\n",
    "from utils.data_utils import get_datasets_dynamically, get_test_datasets_dynamically\n",
    "from utils.settings import Settings\n",
    "import utils.data_evaluation as evaluations\n",
    "\n",
    "from SwinAgeMapper import SwinAgeMapper\n",
    "\n",
    "\n",
    "# Set the default floating point tensor type to FloatTensor\n",
    "torch.set_default_tensor_type(torch.FloatTensor)\n",
    "\n",
    "name = 'SM0-55'\n",
    "\n",
    "\n",
    "settings_file_name = name + '.ini'\n",
    "evaluation_settings_file_name = name + '_eval.ini'\n",
    "\n",
    "settings = Settings(settings_file_name)\n",
    "data_parameters = settings['DATA']\n",
    "training_parameters = settings['TRAINING']\n",
    "network_parameters = settings['NETWORK']\n",
    "misc_parameters = settings['MISC']\n",
    "\n",
    "\n",
    "# network_parameters['feature_size'] = 12\n",
    "# network_parameters['depths'] = (1, 1, 1, 1)\n",
    "\n",
    "batch = 3\n",
    "training_parameters['training_batch_size'] = batch\n",
    "training_parameters['validation_batch_size'] = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_dynamically(data_parameters, mapping_evaluation_parameters=None, flag='train'):\n",
    "    \n",
    "    if flag=='train':\n",
    "        print(\"Data is loading...\")\n",
    "        train_data, validation_data, resolution = get_datasets_dynamically(data_parameters)\n",
    "        print(\"Data has loaded!\")\n",
    "        print(\"Training dataset size is {}\".format(len(train_data)))\n",
    "        print(\"Validation dataset size is {}\".format(len(validation_data)))\n",
    "        return train_data, validation_data, resolution\n",
    "    elif flag=='test':\n",
    "        print(\"Data is loading...\")\n",
    "        test_data, volumes_to_be_used, prediction_output_statistics_name, resolution = get_test_datasets_dynamically(data_parameters, mapping_evaluation_parameters)\n",
    "        print(\"Data has loaded!\")\n",
    "        len_test_data = len(test_data)\n",
    "        print(\"Testing dataset size is {}\".format(len_test_data))\n",
    "        return test_data, volumes_to_be_used, prediction_output_statistics_name, len_test_data, resolution\n",
    "    else:\n",
    "        print('ERROR: Invalid Flag')\n",
    "        return None\n",
    "    \n",
    "train_data, validation_data, resolution = load_data_dynamically(data_parameters=data_parameters, flag='train')\n",
    "\n",
    "if training_parameters['optimiser'] == 'adamw':\n",
    "    optimizer = torch.optim.AdamW\n",
    "elif training_parameters['optimiser'] == 'adam':\n",
    "    optimizer = torch.optim.Adam\n",
    "else:\n",
    "    optimizer = torch.optim.AdamW # Default option\n",
    "\n",
    "optimizer_arguments={'lr': training_parameters['learning_rate'],\n",
    "                    'betas': training_parameters['optimizer_beta'],\n",
    "                    'eps': training_parameters['optimizer_epsilon'],\n",
    "                    'weight_decay': training_parameters['optimizer_weigth_decay']\n",
    "                    }\n",
    "\n",
    "if training_parameters['loss_function'] == 'mse':\n",
    "    loss_function = torch.nn.MSELoss()\n",
    "elif training_parameters['loss_function'] == 'mae':\n",
    "    loss_function = torch.nn.L1Loss()\n",
    "else:\n",
    "    print(\"Loss function not valid. Defaulting to MSE!\")\n",
    "    loss_function = torch.nn.MSELoss()\n",
    "    \n",
    "train_loader = data.DataLoader(\n",
    "    dataset=train_data,\n",
    "    batch_size=training_parameters['training_batch_size'],\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    num_workers=data_parameters['num_workers']\n",
    ")\n",
    "validation_loader = data.DataLoader(\n",
    "    dataset=validation_data,\n",
    "    batch_size=training_parameters['validation_batch_size'],\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    "    num_workers=data_parameters['num_workers']\n",
    ")\n",
    "\n",
    "AgeMapperModel = SwinAgeMapper(\n",
    "                            img_size = network_parameters['img_size'],\n",
    "                            in_channels = network_parameters['in_channels'],\n",
    "                            depths = network_parameters['depths'],\n",
    "                            num_heads = network_parameters['num_heads'],\n",
    "                            feature_size = network_parameters['feature_size'],\n",
    "                            drop_rate = network_parameters['drop_rate'],\n",
    "                            attn_drop_rate = network_parameters['attn_drop_rate'],\n",
    "                            dropout_path_rate = network_parameters['dropout_path_rate'],\n",
    "                            use_checkpoint = network_parameters['use_checkpoint'],\n",
    "                            spatial_dims = network_parameters['spatial_dims'],\n",
    "                            downsample = network_parameters['downsample'],\n",
    "                            fully_connected_activation = network_parameters['fully_connected_activation'],\n",
    "                            resolution=resolution,\n",
    "                            )\n",
    "\n",
    "solver = Solver(model=AgeMapperModel,\n",
    "                number_of_classes=network_parameters['number_of_classes'],\n",
    "                experiment_name=training_parameters['experiment_name'],\n",
    "                optimizer=optimizer,\n",
    "                optimizer_arguments=optimizer_arguments,\n",
    "                loss_function=loss_function,\n",
    "                model_name=training_parameters['experiment_name'],\n",
    "                number_epochs=training_parameters['number_of_epochs'],\n",
    "                loss_log_period=training_parameters['loss_log_period'],\n",
    "                learning_rate_scheduler_gamma=training_parameters['learning_rate_scheduler_gamma'],\n",
    "                use_last_checkpoint=training_parameters['use_last_checkpoint'],\n",
    "                experiment_directory=misc_parameters['experiments_directory'],\n",
    "                logs_directory=misc_parameters['logs_directory'],\n",
    "                checkpoint_directory=misc_parameters['checkpoint_directory'],\n",
    "                best_checkpoint_directory=misc_parameters['best_checkpoint_directory'],\n",
    "                save_model_directory=misc_parameters['save_model_directory'],\n",
    "                learning_rate_scheduler_flag = training_parameters['learning_rate_scheduler_flag'],\n",
    "                learning_rate_scheduler_patience=training_parameters['learning_rate_scheduler_patience'],\n",
    "                learning_rate_scheduler_threshold=training_parameters['learning_rate_scheduler_threshold'],\n",
    "                learning_rate_scheduler_min_value=training_parameters['learning_rate_scheduler_min_value'],\n",
    "                lr_cosine_scheduler_warmup_epochs = training_parameters['lr_cosine_scheduler_warmup_epochs'],\n",
    "                early_stopping_patience=training_parameters['early_stopping_patience'],\n",
    "                early_stopping_min_patience=training_parameters['early_stopping_min_patience'],\n",
    "                early_stopping_min_delta=training_parameters['early_stopping_min_delta'],\n",
    "                use_pre_trained = training_parameters['use_pre_trained'],\n",
    "                )\n",
    "\n",
    "solver.train(train_loader, validation_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import argparse\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "\n",
    "from solver import Solver\n",
    "from utils.data_utils import get_datasets_dynamically, get_test_datasets_dynamically\n",
    "from utils.settings import Settings\n",
    "import utils.data_evaluation as evaluations\n",
    "\n",
    "\n",
    "# Set the default floating point tensor type to FloatTensor\n",
    "torch.set_default_tensor_type(torch.FloatTensor)\n",
    "\n",
    "def load_data_dynamically(data_parameters, mapping_evaluation_parameters=None, flag='train'):\n",
    "    \n",
    "    if flag=='train':\n",
    "        print(\"Data is loading...\")\n",
    "        train_data, validation_data = get_datasets_dynamically(data_parameters)\n",
    "        print(\"Data has loaded!\")\n",
    "        print(\"Training dataset size is {}\".format(len(train_data)))\n",
    "        print(\"Validation dataset size is {}\".format(len(validation_data)))\n",
    "        return train_data, validation_data\n",
    "    elif flag=='test':\n",
    "        print(\"Data is loading...\")\n",
    "        test_data, volumes_to_be_used, prediction_output_statistics_name = get_test_datasets_dynamically(data_parameters, mapping_evaluation_parameters)\n",
    "        print(\"Data has loaded!\")\n",
    "        len_test_data = len(test_data)\n",
    "        print(\"Testing dataset size is {}\".format(len_test_data))\n",
    "        return test_data, volumes_to_be_used, prediction_output_statistics_name, len_test_data\n",
    "    else:\n",
    "        print('ERROR: Invalid Flag')\n",
    "        return None\n",
    "    \n",
    "settings_file_name = 'MMx-1.ini'\n",
    "\n",
    "settings = Settings(settings_file_name)\n",
    "data_parameters = settings['DATA']\n",
    "training_parameters = settings['TRAINING']\n",
    "network_parameters = settings['NETWORK']\n",
    "misc_parameters = settings['MISC']\n",
    "\n",
    "train_data, validation_data = load_data_dynamically(data_parameters=data_parameters, flag='train')\n",
    "\n",
    "train_loader = data.DataLoader(\n",
    "    dataset=train_data,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    "    num_workers=1\n",
    ")\n",
    "\n",
    "i=0\n",
    "for inputs, targets in train_loader:\n",
    "    if i>1:\n",
    "        break\n",
    "    print(type(inputs), type(targets))\n",
    "    print('----------')\n",
    "    i+=1\n",
    "        \n",
    "print(inputs[0][:,80,90,50:55])\n",
    "print(inputs[1][:,80,90,50:55])\n",
    "print(inputs[2][:,80,90,50:55])\n",
    "# print(np.divide(inputs[0][:,80,90,50:55], inputs[1][:,80,90,50:55]))\n",
    "\n",
    "# tensor([[0.6915, 0.5228, 0.5321, 0.5967, 0.6413]])\n",
    "# tensor([[0.4916, 0.6317, 0.6725, 0.6307, 0.5606]])\n",
    "# tensor([[1.4067, 0.8276, 0.7912, 0.9461, 1.1440]])\n",
    "# tensor([[1.4067, 0.8276, 0.7912, 0.9461, 1.1440]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
